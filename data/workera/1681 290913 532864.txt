ntation. The WLASL dataset is freely available for research purposes and has already been used in various studies, including the development of deep learning models for sign language recognition.

In 2020, Dongxu Li et al in the paper titled “Word-level Deep Sign Language Recognition from Video: A New Large-scale Dataset and Methods Comparison”, addressed the challenges faced by existing ASL datasets and built the current largest dataset for the same. Comprising more than 2000 words from over 100 signers, the dataset allows researchers to explore machine learning models that are not limited to only alphabets and digits. Moreover, the dataset contains video signing, allowing the models to consider the motion of hands during translation. [50]

Figure 2 shows two examples from the WLASL dataset (The top row is an ASL sequence for the world ‘rice’, while the bottom row is an ASL sequence for ‘soup’)


Fig 2: frames from the WLASL dataset for rice (top) and soup (bottom). Source: WLASL [50]

Data Preparation
The WLASL dataset needs heavy preprocessing for it to be a fair dataset for the chosen approach. The preprocessing required, and the reasons behind it are discussed in this section. Data preparation is divided into two parts: 1. Video length compression and frame resizing and 2. Data Augmentation. 

First, let’s discuss video length compression. It was observed that each video had varied lengths and hence varied number of frames. The videos were shot at a rate of 25 frames per second (fps) and modeling would require a constant and minimal number of frames per video.  It was also noticed that very few frames were required to encapsulate the sequence. Hence, to eradicate the bias based on length of videos, the videos were either slowed down or sped up to an approximate duration of 3 seconds (giving us a constant 75 frames). Figure 3 shows the video duration before and after this preprocessing step. Out of the 75 frames, 25 equidistant frames were chosen for modeling. Each frame was converted to grayscale and resized to a size of (256,256).


Fig 3: Shows the duration of videos before (left) and after (right) video length compression.

Once 25 frames were chosen from the video, it was important to cater to the class imbalance using data augmentation. Data augmentation is a technique used in machine learning and computer vision to increase the size of a training dataset by creating modified copies of the existing data.

It was observed that over 500 videos had 5 videos per class and over 400 videos had 6 videos per class. To fix the class imbalance, we augmented each video such that we have a minimum of 10 videos per class. Augmentation involves controlled rotation, zooming, flipping and contrast variations. Figure 4 shows skew in number of videos per word vs number of words having those many numbers of videos.



Fig 4: Number of videos per word compared to number of words before data augmentation.

Introduction to Transformers
Transformers are a type of neural network architecture that was introduced in 2017. They are designed for sequence-to-sequence modeling tasks such as language translation, question answering, and text summarization. Unlike traditional recurrent neural networks (RNNs) that process inputs sequentially, transformers process the entire input sequence in parallel, which makes them more efficient and easier to train.

Transformers were introduced in a paper titled “Attention is all you need” in 2017 [51]. The Transformer architecture consists of an encoder and decoder component, each of which contains a stack of identical layers. Each layer has two sub-layers: a multi-head self-attention mechanism and a feedforward network. The self-attention mechanism computes a weighted sum of the input sequence, where the weights are determined by a learned attention function that takes into account the relationships between all pairs of positions in the input sequence. The feedforward network applies a position-wise transformation to the self-attention output, which is then passed to the next layer. During training, the model is optimized to predict the next token in a sequence given the previous tokens, and during inference, the model generates sequences autoregressively. Figure 5 shows a generic transformer architecture.

Vision Transformers (ViT)
Vision Transformers (ViT) is a recently proposed method for image classification that leverages the power of Transformers. While Convolutional Neural Networks (CNNs) have been the dominant approach for image classification, ViT seeks to explore a new way to process images using self-attention mechanisms. Unlike CNNs, ViT processes images as sequences of patches and uses a self-attention mechanism to capture the relationships between patches. The self-attention mechanism enables ViT to focus on the most important patches for classification, allowing it to be more efficient and accurate than traditional CNNs. Vision transformers were 

Fig 5: Transformer architecture

introduced in the paper “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale” by Alexey et al in June 2021. [52] Vision transformers explore the effects of multihead attention on images by dividing it into 16x16 patches and encoding them with position embeddings before passing it through transformer blocks, where it goes through multi head self attention. Figure 6 shows the general architecture of a Vision Transformer.

Vision transformers (ViT) are not the best when it comes to having scarce data. This is exactly the case with the WLASL dataset. Convolution neural networks (CNN), on the other hand, perform well with less data. However, ViT has the ability to embody a larger amount of global information than models like ResNet at the initial layers, leading to comparatively distinct features, and the skip connections in a vision transformer are even more dominant than in Residual Networks, having strong effects on performance. Therefore we combine both CNNs and ViTs for modeling purposes. CNNs come into picture when dealing with feature extraction in our proposed method.

The video vision transformer used in this paper is highly motivated by vision transformers, hence, it must also be noted that the benefits and drawbacks of vision transformers can be extended to Video Vision Transformers since they are heavily based on Vision Transformers. 


Fig 6: Architecture of a vision transformer

Video Vision Transformers
Video Vision Transformer (ViViT) is an extension of the Vision Transformer (ViT) model for video processing tasks such as action recognition, video classification, and video segmentation. The model processes input frames individually through the ViT architecture and then aggregates the output of each frame using temporal attention mechanisms to produce a final video-level prediction. ViViT has achieved state-of-the-art performance on various video datasets and has shown to be computationally efficient compared to traditional 3D CNN-based models.

Based on the success of ViTs, Anurag et al proposed ViViT (A video vision transformer) in 2021 [49]. The pure transformer architecture extracts spatio-temporal tokens from the input video, which are then embedded by a series of transformer layers. The authors experiment with factorizing the spatial and temporal dimensions of the input in different ways. The architecture outline for the video vision transformer is almost the same as figure 6. The only difference is factorizing and how the input video is embedded into tokens. 

Tubelet embeddings are used to embed the video input. It not only divides the image into spatial patches but also includes temporal information in it. The volume frames are then linearly projected and are added with position embeddings before being passed to the transformer layers. Figure 7 shows how an input video is embedded using tubelet embeddings.



Fig 7: How tubelet embedding embeds spatio-temporal information. (Source: ViViT [49])

It must be noted that in our implementation of the ViViT, we make use of 8 transformer encoder layers, a patch size of (8,8,8), 8 heads for attention and a linear projection dimension of 128. 

Experiment 1 - Truncated Resnet and a Shrinker Network
"Deep Residual Learning for Image Recognition" by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun introduced the Residual Network (ResNet) architecture in 2016. ResNet is a deep neural network architecture that enables the construction of deeper neural networks, which were previously difficult to train due to the vanishing gradient problem. ResNet solves this problem by introducing residual connections, allowing gradient to flow directly through the network's layers. In the paper, the authors demonstrated the effectiveness of ResNet on the ImageNet dataset, achieving state-of-the-art results. They also showed that ResNet can be used as a feature extractor and finetuned for various downstream tasks, such as object detection and semantic segmentation. [53]

A truncated pretrained ResNet50 gave the perfect middle ground for less computation but efficient feature extraction. Since the pretrained model was trained on imagenet, it could extract basic important features efficiently from each frame. The output of the model was (25, 32, 32, 512) for each video and it was too huge for the video vision transformer to process given the memory and computing constraints. Hence, we introduce a Shrinker Network, which shrinks the ResNet output to (25, 32,32,1). The outputs are stacked together before sending it to the video vision transformer. Figure 8 shows the rough architecture of the system for experiment 1 while figure 9 shows the detailed architecture of the Shrinker network.


Fig 8: Rough architecture for experiment 1 (ResNet50)




Fig 9: Detailed architecture for the shrinker network


Experiment 2 - Movenet thunder and an Expander Network
MoveNet Thunder is the latest advancement in joint angle profile predictions. The model uses heatmaps to localize human key points. The architecture consists of a MobileNetV2 [54] based features extractor, a feature pyramid network and a set of prediction heads. It was proposed by google’s tensorflow in 2021 and beats state of the art models when it comes to fast accurate pose extraction. The architecture of the MoveNet model is given in figure 10. We make use of tensorflow’s pretrained MoveNet Thunder model which outputs a set of 17 poses. Figure 11 shows the result of applying MoveNet to frames in the WLASL dataset. 


Fig 10: Architecture of the MoveNet Model used for feature extraction in Experiment 2


Fig 11: Result of MoveNet on frames from the WLASL dataset

The output of te MoveNet model is a (17,2) feature vector determining the pose. The video vision transformer however takes images of minimum (16,16). Hence, we introduce an expander model which basically blows up the (17,2) output of the MoveNet to (17,17). We train the expander network such that it takes as input an image of shape (Batch_size, 25,17,2) and outputs an image of shape (Batch_size, 25, 17, 17), which would be passed to the ViViT model for classification. Note that 25 is the number of frames in one video input. 

The rough architecture of the system is given in figure 12, while the architecture of the Expander Network is given in figure 13.


Fig 12: Rough Architecture for experiment 2 (MoveNet)



Fig 13: Architecture of the expander network





IMPLEMENTATION

Configuration
EMBED_DIM = 100
FF_DIM = 32
OUTPUT_SIZE = 100
POSITION_EMBEDDING_SIZE = 10
BATCH_SIZE = 4
VIDEO_SEQUENCE_LENGTH=25
LR_DECAY_RATE=0.96
LR_DECAY_STEPS=100000

AUTO = tf.data.AUTOTUNE
INPUT_SHAPE = (25, 17, 17, 1)
NUM_CLASSES = 100

# OPTIMIZER
LEARNING_RATE = 1e-5
WEIGHT_DECAY = 1e-5

# TRAINING
EPOCHS = 60

# TUBELET EMBEDDING
PATCH_SIZE = (8, 8, 8)
NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2

# ViViT ARCHITECTURE
LAYER_NORM_EPS = 1e-6
PROJECTION_DIM = 128
NUM_HEADS = 8
NUM_LAYERS = 8

PRINT_WIDTH=48

vid_path = "../input/aslamericansignlanguagewlaslpreprocessed"
d100 = "../input/wlasl-divided/WLASL_full/WLASL_full/WLASL100/"
d300 = "../input/wlasl-divided/WLASL_full/WLASL_full/WLASL300/"

d100_train = d100+"train/"
d100_dev = d100+"dev/"
d100_test = d100+"test/"

d100_train_tot = 700
d100_dev_tot = 100
d100_test_tot = 320

d300_train = d300+"train/"
d300_dev = d300+"dev/"
d300_test = d300+"test/"

d300_train_tot = 1800
d300_dev_tot = 300
d300_test_tot = 728


Data Generator
def data_generator(tot, i, path):
    '''
    aim of this would be to output (Batch size, 74, 256, 256)
    '''
    bx_index = i//16
    x = load_data(path+"bx"+str(bx_index)+".npy")
    y = load_data(path+"by"+str(bx_index)+".npy")
    if i+BATCH_SIZE <= tot:
        return i+BATCH_SIZE, x[(i-(16*bx_index)):(i-(16*bx_index))+BATCH_SIZE], y[(i-(16*bx_index)):(i-(16*bx_index))+BATCH_SIZE]
    else:
        return i+BATCH_SIZE, x[(i-(16*bx_index)):tot], y[(i-(16*bx_index)):tot]


Resnet - Exp 1
def build_feature_extractor():
    base_model = ResNet50(weights='imagenet', include_top=True, input_tensor=Input(shape=(256, 256, 3)))
    ResNet14 = Model(inputs = base_model.input,outputs = base_model.get_layer('conv3_block4_add').output)
    ResNet14.trainable = False
    return ResNet14
m = build_feature_extractor()


Shrinker Network
class Shrinker(Model):
    def __init__(self):
        super(Expander, self).__init__()
        self.conv1 = Conv2D(256, kernel_size=(3, 3), strides=(1,1), padding='SAME')
        self.conv2 = Conv2D(80, kernel_size=(3, 3), strides=(1,1), padding='SAME')
        self.conv3 = Conv2D(64, kernel_size=(3, 3), strides=(1,1), padding='SAME')
        self.conv4 = Conv2D(32, kernel_size=(3, 3), strides=(1,1), padding='SAME')
        self.conv5 = Conv2D(1, kernel_size=(1, 1), strides=(1,1), padding='SAME')
        self.bn1 = BatchNormalization()
        self.bn2 = BatchNormalization()
        self.bn3 = BatchNormalization()
        self.bn4 = BatchNormalization()
        self.act1 = Activation('relu')
        self.act2 = Activation('relu')
        self.act3 = Activation('relu')
        self.act4 = Activation('relu')
        self.act5 = Activation('relu')
    
    def call(self, inputs):
        x = self.conv2(inputs)
        x = self.act2(x)
        x = self.bn2(x)
        x = self.conv3(x)
        x = self.act3(x)
        x = self.bn3(x)
        x = self.conv4(x)
        x = self.act4(x)
        x = self.bn4(x)
        x = self.conv5(x)
        x = self.act5(x)
        return x


Movenet - Exp 2
model_name = "movenet_thunder_int8"

if "tflite" in model_name:
  if "movenet_lightning_f16" in model_name:
    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/float16/4?lite-format=tflite
    input_size = 192
  elif "movenet_thunder_f16" in model_name:
    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/float16/4?lite-format=tflite
    input_size = 256
  elif "movenet_lightning_int8" in model_name:
    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/int8/4?lite-format=tflite
    input_size = 192
  elif "movenet_thunder_int8" in model_name:
    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/int8/4?lite-format=tflite
    input_size = 256
  else:
    raise ValueError("Unsupported model name: %s" % model_name)

  # Initialize the TFLite interpreter
  interpreter = tf.lite.Interpreter(model_path="model.tflite")
  interpreter.allocate_tensors()

  def movenet(input_image):
    input_image = tf.cast(input_image, dtype=tf.uint8)
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    interpreter.set_tensor(input_details[0]['index'], input_image.numpy())
    interpreter.invoke()
    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])
    return keypoints_with_scores

else:
  if "movenet_lightning" in model_name:
    module = hub.load("https://tfhub.dev/google/movenet/singlepose/lightning/4")
    input_size = 192
  elif "movenet_thunder" in model_name:
    module = hub.load("https://tfhub.dev/google/movenet/singlepose/thunder/4")
    input_size = 256
  else:
    raise ValueError("Unsupported model name: %s" % model_name)

  def movenet(input_image):
    model = module.signatures['serving_default']
    input_image = tf.cast(input_image, dtype=tf.int32)
    outputs = model(input_image)
    keypoints_with_scores = outputs['output_0'].numpy()
    return keypoints_with_scores


Expander Network
class Expander(Model):
    def __init__(self):
        super(Expander, self).__init__()
        self.conv1 = Conv2D(256, kernel_size=(3, 3), strides=(1,1), padding='SAME')
        self.conv6 = Conv2D(256, kernel_size=(3, 3), strides=(1,1), padding='SAME')
        self.conv2 = Conv2D(128, kernel_size=(3, 3), strides=(1,1), padding='SAME')
        self.conv3 = Conv2D(64, kernel_size=(3, 3), strides=(1,1), padding='SAME')
        self.conv4 = Conv2D(32, kernel_size=(3, 3), strides=(1,1), padding='SAME')
        self.conv5 = Conv2D(17, kernel_size=(3, 3), strides=(1,1), padding='SAME')
        self.bn1 = BatchNormalization()
        self.bn2 = BatchNormalization()
        self.bn3 = BatchNormalization()
        self.bn4 = BatchNormalization()
        self.bn5 = BatchNormalization()
        self.act1 = Activation('relu')
        self.act2 = Activation('relu')
        self.act3 = Activation('relu')
        self.act4 = Activation('relu')
        self.act5 = Activation('relu')
        self.act6 = Activation('relu')
    
    def call(self, inputs):
        x = self.conv1(inputs)
        x = self.act1(x)
        x = self.bn1(x)
        
        x = self.conv6(x)
        x = self.act6(x)
        x = self.bn5(x)
        
        x = self.conv2(x)
        x = self.act2(x)
        x = self.bn2(x)
        
        x = self.conv3(x)
        x = self.act3(x)
        x = self.bn3(x)
        
        x = self.conv4(x)
        x = self.act4(x)
        x = self.bn4(x)
        
        x = self.conv5(x)
        x = self.act5(x)
        return x


Vision Transformer
class TubeletEmbedding(layers.Layer):
    def __init__(self, embed_dim, patch_size, **kwargs):
        super().__init__(**kwargs)
        self.projection = layers.Conv3D(
            filters=embed_dim,
            kernel_size=patch_size,
            strides=patch_size,
            padding="VALID",
        )
        self.flatten = layers.Reshape(target_shape=(-1, embed_dim))

    def call(self, videos):
        projected_patches = self.projection(videos)
        flattened_patches = self.flatten(projected_patches)
        return flattened_patches
    
class PositionalEncoder(layers.Layer):
    def __init__(self, embed_dim, **kwargs):
        super().__init__(**kwargs)
        self.embed_dim = embed_dim

    def build(self, input_shape):
        _, num_tokens, _ = input_shape
        self.position_embedding = layers.Embedding(
            input_dim=num_tokens, output_dim=self.embed_dim
        )
        self.positions = tf.range(start=0, limit=num_tokens, delta=1)

    def call(self, encoded_tokens):
        # Encode the positions and add it to the encoded tokens
        encoded_positions = self.position_embedding(self.positions)
        encoded_tokens = encoded_tokens + encoded_positions
        return encoded_tokens
    
class ViViT(Model):
    def __init__(self):
        super(ViViT, self).__init__()
        self.tubelet_embedder=TubeletEmbedding(embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE)
        self.positional_encoder = PositionalEncoder(embed_dim=PROJECTION_DIM)
        self.ln1 = layers.LayerNormalization(epsilon=1e-6)
        self.ln2 = layers.LayerNormalization(epsilon=1e-6)
        self.ln3 = layers.LayerNormalization(epsilon=1e-6)
        self.ln4 = layers.LayerNormalization(epsilon=1e-6)
        self.ln5 = layers.LayerNormalization(epsilon=1e-6)
        self.ln6 = layers.LayerNormalization(epsilon=1e-6)
        self.ln7 = layers.LayerNormalization(epsilon=1e-6)
        self.ln8 = layers.LayerNormalization(epsilon=1e-6)
        self.ln9 = layers.LayerNormalization(epsilon=1e-6)
        self.ln10 = layers.LayerNormalization(epsilon=1e-6)
        self.ln11 = layers.LayerNormalization(epsilon=1e-6)
        self.ln12 = layers.LayerNormalization(epsilon=1e-6)
        self.ln13 = layers.LayerNormalization(epsilon=1e-6)
        self.ln14 = layers.LayerNormalization(epsilon=1e-6)
        self.ln15 = layers.LayerNormalization(epsilon=1e-6)
        self.ln16 = layers.LayerNormalization(epsilon=1e-6)
        self.lnfinal = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)
        self.multihead1 = layers.MultiHeadAttention(num_heads=NUM_HEADS, key_dim=PROJECTION_DIM // NUM_HEADS, dropout=0.1)
        self.multihead2 = layers.MultiHeadAttention(num_heads=NUM_HEADS, key_dim=PROJECTION_DIM // NUM_HEADS, dropout=0.1)
        self.multihead3 = layers.MultiHeadAttention(num_heads=NUM_HEADS, key_dim=PROJECTION_DIM // NUM_HEADS, dropout=0.1)
        self.multihead4 = layers.MultiHeadAttention(num_heads=NUM_HEADS, key_dim=PROJECTION_DIM // NUM_HEADS, dropout=0.1)
        self.multihead5 = layers.MultiHeadAttention(num_heads=NUM_HEADS, key_dim=PROJECTION_DIM // NUM_HEADS, dropout=0.1)
        self.multihead6 = layers.MultiHeadAttention(num_heads=NUM_HEADS, key_dim=PROJECTION_DIM // NUM_HEADS, dropout=0.1)
        self.multihead7 = layers.MultiHeadAttention(num_heads=NUM_HEADS, key_dim=PROJECTION_DIM // NUM_HEADS, dropout=0.1)
        self.multihead8 = layers.MultiHeadAttention(num_heads=NUM_HEADS, key_dim=PROJECTION_DIM // NUM_HEADS, dropout=0.1)
        self.add1 = layers.Add()
        self.add2 = layers.Add()
        self.add3 = layers.Add()
        self.add4 = layers.Add()
        self.add5 = layers.Add()
        self.add6 = layers.Add()
        self.add7 = layers.Add()
        self.add8 = layers.Add()
        self.add9 = layers.Add()
        self.add10 = layers.Add()
        self.add11 = layers.Add()
        self.add12 = layers.Add()
        self.add13 = layers.Add()
        self.add14 = layers.Add()
        self.add15 = layers.Add()
        self.add16 = layers.Add()
        self.densea1 = layers.Dense(units=PROJECTION_DIM * 4, activation=tf.nn.gelu)
        self.densea2 = layers.Dense(units=PROJECTION_DIM * 4, activation=tf.nn.gelu)
        self.densea3 = layers.Dense(units=PROJECTION_DIM * 4, activation=tf.nn.gelu)
        self.densea4 = layers.Dense(units=PROJECTION_DIM * 4, activation=tf.nn.gelu)
        self.densea5 = layers.Dense(units=PROJECTION_DIM * 4, activation=tf.nn.gelu)
        self.densea6 = layers.Dense(units=PROJECTION_DIM * 4, activation=tf.nn.gelu)
        self.densea7 = layers.Dense(units=PROJECTION_DIM * 4, activation=tf.nn.gelu)
        self.densea8 = layers.Dense(units=PROJECTION_DIM * 4, activation=tf.nn.gelu)
        self.denseb1 = layers.Dense(units=PROJECTION_DIM, activation=tf.nn.gelu)
        self.denseb2 = layers.Dense(units=PROJECTION_DIM, activation=tf.nn.gelu)
        self.denseb3 = layers.Dense(units=PROJECTION_DIM, activation=tf.nn.gelu)
        self.denseb4 = layers.Dense(units=PROJECTION_DIM, activation=tf.nn.gelu)
        self.denseb5 = layers.Dense(units=PROJECTION_DIM, activation=tf.nn.gelu)
        self.denseb6 = layers.Dense(units=PROJECTION_DIM, activation=tf.nn.gelu)
        self.denseb7 = layers.Dense(units=PROJECTION_DIM, activation=tf.nn.gelu)
        self.denseb8 = layers.Dense(units=PROJECTION_DIM, activation=tf.nn.gelu)
        self.densefinal = layers.Dense(units=NUM_CLASSES, activation="softmax")
        self.gap = layers.GlobalAveragePooling1D()
    
    def call(self, inputs):
        patches = self.tubelet_embedder(inputs)
        encoded_patches = self.positional_encoder(patches)
        
        x1 = self.ln1(encoded_patches)
        attention_output = self.multihead1(x1,x1)
        x2 = self.add1([attention_output, encoded_patches])
        x3 = self.ln2(x2)
        x3 = self.densea1(x3)
        x3 = self.denseb1(x3)
        encoded_patches = self.add2([x3,x2])
        
        x1 = self.ln3(encoded_patches)
        attention_output = self.multihead2(x1,x1)
        x2 = self.add3([attention_output, encoded_patches])
        x3 = self.ln4(x2)
        x3 = self.densea2(x3)
        x3 = self.denseb2(x3)
        encoded_patches = self.add4([x3,x2])
        
        x1 = self.ln5(encoded_patches)
        attention_output = self.multihead3(x1,x1)
        x2 = self.add5([attention_output, encoded_patches])
        x3 = self.ln6(x2)
        x3 = self.densea3(x3)
        x3 = self.denseb3(x3)
        encoded_patches = self.add6([x3,x2])
        
        x1 = self.ln7(encoded_patches)
        attention_output = self.multihead4(x1,x1)
        x2 = self.add7([attention_output, encoded_patches])
        x3 = self.ln8(x2)
        x3 = self.densea4(x3)
        x3 = self.denseb4(x3)
        encoded_patches = self.add8([x3,x2])
        
        x1 = self.ln9(encoded_patches)
        attention_output = self.multihead5(x1,x1)
        x2 = self.add9([attention_output, encoded_patches])
        x3 = self.ln10(x2)
        x3 = self.densea5(x3)
        x3 = self.denseb5(x3)
        encoded_patches = self.add10([x3,x2])
        
        x1 = self.ln11(encoded_patches)
        attention_output = self.multihead6(x1,x1)
        x2 = self.add11([attention_output, encoded_patches])
        x3 = self.ln12(x2)
        x3 = self.densea6(x3)
        x3 = self.denseb6(x3)
        encoded_patches = self.add12([x3,x2])
        
        x1 = self.ln13(encoded_patches)
        attention_output = self.multihead7(x1,x1)
        x2 = self.add13([attention_output, encoded_patches])
        x3 = self.ln14(x2)
        x3 = self.densea7(x3)
        x3 = self.denseb7(x3)
        encoded_patches = self.add14([x3,x2])
        
        x1 = self.ln15(encoded_patches)
        attention_output = self.multihead8(x1,x1)
        x2 = self.add15([attention_output, encoded_patches])
        x3 = self.ln16(x2)
        x3 = self.densea8(x3)
        x3 = self.denseb8(x3)
        encoded_patches = self.add16([x3,x2])
        
        representation = self.lnfinal(encoded_patches)
        representation = self.gap(representation)
        outputs = self.densefinal(representation)
        return outputs
        
Train Loop
def train(epochs, train_data_path, dev_data_path, vivit,expander, total_train, total_dev):
    for e in range(epochs):
        
        print("epoch:",e)
        
        st = time.time() #start timer        
        idx = 0
        loss_train=0
        acc_train = (0.0, 0.0, 0.0)
        while idx<total_train-BATCH_SIZE:
            if idx%PRINT_WIDTH == 0:
                print("batch t:", str(idx), end=" ")
                
            with tf.GradientTape() as tape:
                idx, x, y = data_generator(total_train, idx, train_data_path)
                o = expander_model(x)
                o = tf.expand_dims(o,axis=4)
                outputs = vivit(o)
                ll, loss = loss_function(y,outputs)
                acc = accuracy(y, outputs)
                acc_train = tuple(map(lambda x, y: x+(y*BATCH_SIZE), acc_train, acc))
            loss_train+=loss
            variables = vivit.trainable_variables + expander.trainable_variables
            gradients = tape.gradient(loss,variables)                        
            optimizer.apply_gradients(zip(gradients, variables))
        total_loss_train.append(loss_train/total_train)
        acc_train = [i/total_train for i in acc_train]
        total_acc_train.append(acc_train)
        
        print()
        print("train loss:", str(loss_train/total_train))
        print("train accuracy:", str(acc_train))
        print("time:", time.time()-st)
        print()
        
        st = time.time()
        idx = 0
        loss_dev=0
        acc_dev = (0.0, 0.0, 0.0)
        while idx<total_dev:
            if idx%PRINT_WIDTH == 0:
                print("batch d:", str(idx), end=" ")
            idx, x, y = data_generator(total_dev, idx, dev_data_path)
            o = expander_model(x)
            o = tf.expand_dims(o,axis=4)
            outputs = vivit(o)
            ll, loss = loss_function(y, outputs)
            acc = accuracy(y, outputs)
            acc_dev = tuple(map(lambda x, y: x+(y*BATCH_SIZE), acc_dev, acc))
            loss_dev+=loss
        total_loss_val.append(loss_dev/total_dev)
        acc_dev = [i/total_dev for i in acc_dev]
        total_acc_val.append(acc_dev)
    
        print()
        print("dev loss:", str(loss_dev/total_dev))
        print("dev accuracy:", str(acc_dev))
        print("time:", time.time()-st)
        print()
        print("===================================================================================================================================")
        print()

    return vivit, expander, total_acc_train, total_acc_val, total_loss_val, total_loss_train















RESULTS

Experimental Setup
Table 1 shows the experimental setup for execution of the experiments


CPU RAM
13 GB
GPU RAM
16 GB
Graphics Processor
Tesla T4
Language
Python 3.8.1


Table 1: Experimental Setup

Results
Both experiment 1 (Truncated ResNet50) and experiment 2 (MoveNet Thunder) perform well on the WLASL100 and WLASL300 dataset, however it is seen that the ResNet50 variation fails to generalize well. Although the training curve surpasses few of the baseline models described in the paper introducing the WLASL dataset, the validation and test results don’t compliment it. However, experiment 2 seems to be comparatively promising and performs well on all train, dev and test datasets. Experiment 1 gives a train accuracy of 58.72% while giving a test accuracy of 43.25% on the WLASL100 dataset. Experiment 2 on the other hand gives a train accuracy of 61.38% and a test accuracy of 59.44% on the WLASL100 dataset. As we move on to WLASL 300, the trends remain the same whereas the accuracies drop as a result of increased number of target classes. 


Method
WLASL100 (%)
WLASL300 (%)


Top-1
Top-5
Top-10
Top-1
Top-5
Top-10
Exp 1 (train)
58.72
83.84
91.35
41.92
69.22
80.07
Exp 1 (test)
43.25
70.79
79.15
30.14
55.03
68.47
Exp 2 (train)
61.38
80.36
90.03
51.40
74.06
85.68
Exp 2 (test)
59.44
78.97
89.23
48.38
72.59
86.81


Table 2: Train and Test accuracies on WLASL100 and WLASL300 for experiment 1 (denoted by Exp1) and experiment 2 (denoted by Exp2)



For training, the loss function was set to be the usual categorical cross entropy. A learning rate of 1e-5 was used and an exponential learning rate decay with a decay rate of 0.96 and a decay step of 1e5 was applied. For experiment 2, a batch size of 16 was used due to it being computationally cost effective while a batch size of 4 was used in experiment 1, which was computationally heavy. Furthermore, it is to be noted that both the experiments are performed in the computational constraints defined by Table 1 and the results may be dependent on them.


Fig 14: Training curve for the Movenet variant (experiment 2)



FIg 15: Training curve for the ResNet Variant (experimet 1)


The training curve for experiment 2 is shown in figure 14 and the top-1, top-2 and top-10 accuracies on the train and test datasets are displayed in table 2. The MoveNet variant does better than Pose-GRU, Pose-TGCN and VGG-GRU and the evidence for the same is shown in table 3. The training curve for experiment 1 is shown in figure 15.


Method
WLASL100 (%)
WLASL300 (%)


Top-1
Top-5
Top-10
Top-1
Top-5
Top-10
Pose-GRU
46.51
76.74
85.66
33.68
64.37
76.05
Pose-TGCN
55.43
78.68
87.60
38.32
67.51
79.64
VGG-GRU
25.97
55.04
63.95
19.31
46.56
61.08
Exp 1 (test)
43.25
70.79
79.15
30.14
55.03
68.47
Exp 2 (test)
59.44
78.97
89.23
48.38
72.59
86.81


Table 3: Comparing results of previous state of the art models on the WLASL100 and WLASL300 dataset with test accuracies of experiment 1 (denoted by Exp1) and experiment 2 (denoted by Exp2).





















CONCLUSION

We propose a new efficient method for user independent translation from ASL to english words. A deep learning approach to vision based translation is employed to translate a video feed into a set of known english words. The main objective of the paper is to explore rising deep learning technologies like pose detection using MoveNet and video classification using video vision transformers.

The paper experiments using two feature extractors (MoveNet Thunder and a truncated pretrained ResNet50) and introduces for each of the experiments an expander and a shrinker network which allow us to leverage the spatiotemporal advantages of video vision transformers in a computation friendly manner.

While the ResNet50 model fails to generalize well, the MoveNet variant predicts with an accuracy better than previous state of the art models like Pose-GRU, Pose-TGCN and VGG-GRU. The MoveNet model achieves a top-1 accuracy of 59.44% for the WLASL100 dataset and a top-1 accuracy of 48.38% for the WLASL300 dataset.

Future work involves training in a computational environment friendly enough to train the WLASL1000 and WLASL2000 subsets of the WLASL dataset. Furthermore, it is also possible to experiment with different patch sizes and transformer embedder layers to see its effects on the results.


















REFERENCES

[1]	B. J. Kröger, P. Birkholz, J. Kannampuzha, E. Kaufmann, and I. Mittelberg, ‘Movements and holds in fluent sentence production of American Sign Language: The action-based approach’, Cognitive computation, vol. 3, no. 3, pp. 449–465, 2011

[2]	N. Gupta and N. Jhanwar, "A Comparative Study of Body Language and Sign Language," International Journal of Computer Science and Mobile Computing, vol. 4, no. 4, pp. 200-207, Apr. 2015

[3]	E. Wilkinson and K. S. Schmitz, "The Role of Facial Expressions in Sign Language and Body Language," Sign Language Studies, vol. 18, no. 4, pp. 558-576, 2018.

[4]	K. Goudarzi and K. Tsivola, "Cross-Cultural Differences in Body Language and Sign Language," Journal of Language and Social Psychology, vol. 36, no. 3, pp. 317-330, Jun. 2017

[5]	J. Napier and L. Leeson, ‘Sign language in action’, in Sign language in action, Springer, 2016, pp. 50–84

[6]	Padden, C. A., & Humphries, T. L. (2005). A brief history of American Sign Language. In D. F. Armstrong, M. A. Karchmer, & J. V. Van Cleve (Eds.), The study of signed languages: Essays in honor of William C. Stokoe (pp. 3-24). Gallaudet University Press

[7]	D. Lillo-Martin, "The Syntax of American Sign Language: Functional Categories and Hierarchical Structure," in Natural Language & Linguistic Theory, vol. 10, no. 3, pp. 395-429, Sep. 1992, doi: 10.1007/BF00992726

[8]	K. Emmorey, "Syntactic and Semantic Factors in the Use of Space in American Sign Language," in Language and Cognitive Processes, vol. 10, no. 1, pp. 31-56, 1995, doi: 10.1080/01690969508407076

[9]	Bahan, B., & Bauman, H. D. (2018). American Sign Language. In Oxford Research Encyclopedia of Linguistics. doi: 10.1093/acrefore/9780199384655.013.204.

[10]	de Quadros, R. M. (2014). Sign languages of the world: A comparative handbook. Cambridge University Press

[11]	Emmorey, K. (2002). Language, cognition, and the brain: Insights from sign language research. Psychology Press

[12]	H. Lane, "When the mind hears: A history of the Deaf," Random House, 1984.

[13]	Stokoe, W. C. (1960). Sign language structure: An outline of the visual communication

[14]	M. Marschark and P. E. Spencer, The Oxford handbook of deaf studies, language, and education, vol. 2. Oxford University Press, 2010

[15]	S. Goldin-Meadow, D. Brentari and M.A. Easterbrooks, "Sign Language and Academic Achievement: Evidence from a Longitudinal Study," Sign Language Studies, vol. 14, no. 1, pp. 18-36, 2013. doi: 10.1353/sls.2013.0003

[16]	S. B. Waltzman and A. Blum, "Barriers to Health Care Access for Deaf and Hard-of-Hearing Individuals," Annals of Internal Medicine, vol. 130, no. 5, pp. 392-399, Mar. 1999. doi: 10.7326/0003-4819-130-5-199903020-00011

[17]	R. Adam, C. McNeill, and E. Rigby, "Sign Language Interpreters in Health Care Settings: A Systematic Review of the International Literature," Journal of Deaf Studies and Deaf Education, vol. 18, no. 3, pp. 333-351, Jul. 2013. doi: 10.1093/deafed/ent010

[18]	L. A. Tucci, W. Myhill, and B. Tucker, "Justice Denied: The Barriers to Legal Services for Persons with Disabilities," Boston, MA: Boston College Law School, 2006

[19]	J. Napier and R. Skinner, "Deaf People and the Criminal Justice System," in Sign Language Studies, vol. 7, no. 4, pp. 303-327, 2007

[20]	J. R. Beacham, J. C. DeJong, and L. J. Berkeley, "The Impact of Communication Barriers on the Mental Health of Deaf and Hard of Hearing Individuals," Journal of Mental Health Counseling, vol. 34, no. 1, pp. 56-69, 2012

[21]	P. Hindley and M. Hill, "Deafness and Mental Health: An Overview," British Journal of Psychiatry, vol. 190, pp. 1-4, 2007.

[22]	H. Kumar, M. Kumar, and A. Ranjan, "Real-time American Sign Language Recognition Using Deep Learning Models," 2018 IEEE 4th International Conference on Computational Intelligence and Applications (ICCIA), 2018, pp. 1-6, doi: 10.1109/ICCIA.2018.8662008

[23]	A. J. Reynolds and D. J. Lubin, "Gloves that Translate American Sign Language into English Speech," in Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems (CHI EA '16), 2016, pp. 2992-2999, doi: 10.1145/2851581.2856491

[24]	R. Bajpai and D. Joshi, ‘Movenet: A deep neural network for joint profile prediction across variable walking speeds and slopes’, IEEE Transactions on Instrumentation and Measurement, vol. 70, pp. 1–11, 2021

[25]	A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Lučić, and C. Schmid, ‘Vivit: A video vision transformer’, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 6836–6846

[26]	C. K. M. Lee, K. K. H. Ng, C.-H. Chen, H. C. W. Lau, S. Y. Chung, and T. Tsoi, "American sign language recognition and training method with recurrent neural network," Expert Systems with Applications, vol. 167, p. 114403, 2021

[27]	D. Avola, M. Bernardi, L. Cinque, G. L. Foresti, and C. Massaroni, ‘Exploiting recurrent neural networks and leap motion controller for the recognition of sign language and semaphoric hand gestures’, IEEE Transactions on Multimedia, vol. 21, no. 1, pp. 234–245, 2018

[28]	R. Gouider, O. K. H. Chelly, and A. M. Alimi, "Sign language recognition using Leap Motion and K-means clustering," 2015 15th International Conference on Intelligent Systems Design and Applications (ISDA), 2015, pp. 376-382, doi: 10.1109/ISDA.2015.7489173

[29]	M. I. B. Shamsul, M. R. M. Saad, and M. F. Zainal Abidin, "Sign Language Recognition System using Leap Motion Sensor," 2016 International Conference on Computer and Communication Engineering (ICCCE), Kuala Lumpur, 2016, pp. 147-152, doi: 10.1109/ICCCE.2016.64

[30]	C. Öz and M. C. Leu, "American Sign Language word recognition with a sensory glove using artificial neural networks," IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), vol. 29, no. 6, pp. 752-758, 1999

[31]	N. Bajaj and A. Kumar, "A Sensor-Based Approach for Real-Time Sign Language Recognition and Translation," in Proceedings of the 7th International Conference on Signal Processing and Communication Systems (ICSPCS), 2013, pp. 1-7, doi: 10.1109/ICSPCS.2013.6723979

[32]	J. K. Perez, K. R. Moallemi, and S. R. Kang, "A Low-Cost Glove-Based System for American Sign Language Translation," in IEEE Transactions on Consumer Electronics, vol. 56, no. 2, pp. 673-680, May 2010, doi: 10.1109/TCE.2010.5506357

[33]	S. Ghosh and K. Mitra, "A Glove-Based System for American Sign Language Translation Using Fuzzy Inference," in Proceedings of the International Conference on Information Technology and Knowledge Management, 2010, pp. 98-103. doi: 10.1145/1906165.1906191

[34]	S. S. Mahajan, S. K. Mitra, and S. K. Parui, "A Comprehensive Study on Gesture Recognition Using Surface EMG and Machine Learning Techniques," in IEEE Access, vol. 8, pp. 182551-182568, 2020, doi: 10.1109/ACCESS.2020.3023255

[35]	R. Zangeneh, M. Shahid, and M. A. Islam, "Sign Language Recognition using EMG Sensors and Artificial Neural Networks," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 4603-4608, doi: 10.1109/ICRA.2019.8793636

[36]	K. Adarsh, S. K. Mitra, and S. K. Parui, "Sign Language Recognition using EMG Signals and Convolutional Neural Networks," 2020 IEEE International Conference on Communication and Signal Processing (ICCSP), Chennai, India, 2020, pp. 0158-0162, doi: 10.1109/ICCSP48518.2020.9198832

[37]	S. Shaghayegh and S. Mirshekari, "Sign Language Recognition Based on EMG Signals and Hidden Markov Models," 2020 11th International Conference on Information and Communication Systems (ICICS), Singapore, Singapore, 2020, pp. 225-230, doi: 10.1109/IACS51288.2020.9328624

[38]	W. Aly, S. Aly, and S. Almotairi, ‘User-independent American sign language alphabet recognition based on depth image and PCANet features’, IEEE Access, vol. 7, pp. 123138–123150, 2019

[39]	R. Ahuja, D. Jain, D. Sachdeva, A. Garg, and C. Rajput, ‘Convolutional neural network based american sign language static hand gesture recognition’, International Journal of Ambient Computing and Intelligence (IJACI), vol. 10, no. 3, pp. 60–73, 2019

[40]	R. Kaluri and C. H. Pradeep, ‘An enhanced framework for sign gesture recognition using hidden Markov model and adaptive histogram technique’, International Journal of Intelligent Engineering and Systems, vol. 10, no. 3, pp. 11–19, 2017

[41]	V. Bheda and D. Radpour, ‘Using deep convolutional networks for gesture recognition in american sign language’, arXiv preprint arXiv:1710. 06836, 2017

[42]	D. Metaxas, M. Dilsizian, and C. Neidle, ‘Scalable ASL sign recognition using model-based machine learning and linguistically annotated corpora’, in sign-lang@ LREC 2018, 2018, pp. 127–132

[43]	C. Neidle, A. Thangali, and S. Sclaroff, ‘Challenges in development of the american sign language lexicon video dataset (asllvd) corpus’, in 5th workshop on the representation and processing of sign languages: interactions between corpus and Lexicon, LREC, 2012.

[44]	J. Huang, W. Zhou, H. Li, and W. Li, "Sign Language Recognition Using 3D Convolutional Neural Networks," in IEEE Transactions on Neural Networks and Learning Systems, vol. 31, no. 10, pp. 3716-3729, Oct. 2020, doi: 10.1109/TNNLS.2019.2958671

[45]	M. Oberweger, P. Wohlhart, and V. Lepetit, "A database for hand gesture recognition based on multiple cues and subjects," in Proceedings of the 2015 IEEE International Conference on Computer Vision Workshop (ICCVW), Santiago, Chile, 2015, pp. 42-49

[46]	H. Huang and P. Hao, "Sign Language Recognition Using Convolutional Neural Networks and Temporal Pyramid Matching," in IEEE Transactions on Human-Machine Systems, vol. 47, no. 5, pp. 661-670, Oct. 2017, doi: 10.1109/THMS.2016.2621439

[47]	Doan, T. T., Rho, S., & Kim, N. (2019). Deep Learning-Based American Sign Language Recognition Using Spatiotemporal Features from Video Data. Sensors, 19(17), 3677. DOI: 10.3390/s19173677

[48]	R. Bajpai and D. Joshi, ‘Movenet: A deep neural network for joint profile prediction across variable walking speeds and slopes’, IEEE Transactions on Instrumentation and Measurement, vol. 70, pp. 1–11, 2021.

[49]	A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Lučić, and C. Schmid, ‘Vivit: A video vision transformer’, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 6836–6846.

[50]	D. Li, C. Rodriguez, X. Yu, and H. Li, ‘Word-level deep sign language recognition from video: A new large-scale dataset and methods comparison’, in Proceedings of the IEEE/CVF winter conference on applications of computer vision, 2020, pp. 1459–1469.

[51]	Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing systems (pp. 5998-6008).

[52]	D. Dosovitskiy et al., "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021, pp. 10685-10695, doi: 10.1109/CVPR.2021.01152.

[53]	K. He, X. Zhang, S. Ren and J. Sun, "Deep Residual Learning for Image Recognition," 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, USA, 2016, pp. 770-778, doi: 10.1109/CVPR.2016.90.

[54]	Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., Adam, H. (2017). MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 1387-1396).

ABSTRACT

Sign Language is a visual language that uses manual communication and body language to convey meaning. It is the most natural and effective way for communication amongst the hearing/vocally challenged and the hearing abled. However, there is a huge social gap that the deaf and the mute face due to the lack of resources available for recognizing sign language. This can lead to isolation and a feeling of being disconnected from the larger community. 

Moreover, despite the recognition of sign language as a legitimate form of communication, it has often been stigmatized and dismissed as an inferior form of communication. Many Deaf people have faced discrimination and prejudice due to their use of sign language, and have been denied access to education and other opportunities. To overcome this bias, sign language has gained popularity and is now given the same linguistic status as of spoken languages. However, this has caused the deaf and vocally impaired sign language users to be considered as members of a cultural and linguistic minority group instead of a disability group. With such a bipartite distinction, it is a challenge for the disabled signers to identify themselves and weave smoothly into the social fabric

Sign language recognition technology can help bridge this gap by enabling communication between hearing individuals and those who use sign language as their primary mode of communication. With the pace at which Machine Learning is growing, newer and better methods can be employed to automatically recognize sign language. 

Hence, we propose a system that aids in translating from a video feed of American Sign Language (ASL) signers to a known set of common english words. Current solutions for vision based sign language to english translation is limited to a very small set of target classes and old modeling techniques that do not take into consideration the temporal factor that caters to the sequential nature of signing. We use the WLASL dataset (A large-scale dataset for Word-Level American Sign Language), which comprises 2000 words from over 100 signers and develop models on WLASL100 and WLASL300 (each representing the top 100 and 300 most commonly appearing glossaries respectively). For modelling, we make use of video vision transformers (ViViT) to leverage the power of multi head attention in a temporal and spatial setting and experiment with two feature extraction models (Truncated Resnet50 and MoveNet-thunder) which act as a precursor to the ViViT.  The final model does better than state of the art models like Pose-GRU, Pose-TGCN and VGG-GRU (which were also trained on the WLASL dataset) with a top-1 accuracy of 59.44% on the WLASL100 subset and a top-1 accuracy of 48.38% on the WLASL300 subset.



INTRODUCTION

Sign language is a visual-gestural language that uses hand movements, facial expressions, and body language to communicate meaning. It is a language used by Deaf and hard of hearing people and is recognized as a natural and complete language, just like any spoken language. Sign language has its own grammar, syntax, and vocabulary, and it is not simply a visual representation of a spoken language. The language makes use of sequential and structured body movements, which are specific to the sign language used, to convey thoughts and speech. To understand the importance of the language, it is essential to study communication in general. The study of speech, sound or any form of communication can be done by dividing it into three different systems: The manual system, verbal system, and the facial system [1]. 

Manual systems of communication through limb movements, also known as gesture-based communication, have been in use for centuries. These systems involve the use of hand, arm, and body movements to convey meaning. One of the primary advantages of manual communication systems based on limb movements is that they can be used to communicate in situations where spoken language is not practical or possible. For example, in loud environments or when communicating over long distances, limb movements can be used to convey meaning without the need for vocal communication.Another benefit of manual communication systems based on limb movements is their simplicity. They can be learned quickly and easily, and require no special equipment or infrastructure. This makes them accessible to anyone, regardless of their education or socioeconomic status. However, manual communication systems based on limb movements also have some limitations. They can be slow and may not be able to transmit as much information as quickly or efficiently as spoken language or other forms of communication. 

Verbal communication is a system of communication that relies on spoken language to convey meaning between individuals. It is the most commonly used method of communication and plays a critical role in our daily lives. One of the primary benefits of verbal communication is its efficiency. Spoken language can convey a vast amount of information quickly and accurately. Additionally, it can be used to express complex ideas, thoughts, and emotions, making it a powerful tool for interpersonal communication. However, there are also some drawbacks to verbal communication. One of the primary limitations is that it can be subject to misinterpretation. Spoken language relies on tone, inflection, and context to convey meaning, and these elements can be easily misinterpreted or misunderstood. Additionally, verbal communication can be influenced by factors such as culture, dialect, and accent, which can further complicate the process of communication.

Facial communication is a system of communication that relies on facial expressions and movements to convey meaning between individuals. This system of communication is an essential aspect of human communication, as it allows individuals to convey emotions, attitudes, and intentions without the need for spoken language. One of the primary benefits of facial communication is its universality. Facial expressions are recognized and understood across cultures, making it a powerful tool for interpersonal communication. Additionally, facial expressions are often instinctual and can be used to convey emotions and attitudes quickly. However, facial communication can be influenced by cultural norms and expectations. Different cultures may have different expectations regarding the appropriate use of facial expressions, which can lead to misunderstandings or misinterpretations in cross-cultural communication.

Upon studying languages, it is found that in spoken language, the verbal system dominates while in sign languages, the manual system dominates [1]. The manual system here, however, is not to be confused with body language, which is a non-verbal form of communication. Both forms of communication (sign language and body language) involve nonverbal cues but are fundamentally different [2]. Facial expressions, on the other hand, are important in both forms of communication, but they play a more central role in sign language due to its visual nature [3]. It is argued that while some nonverbal cues may be universally understood, others are culturally specific and require a deeper understanding of the local culture [4]. Sign languages, being part of the non verbal cues which depend on cultural and regional aspects, are not mutually nor universally comprehensible [5]. One of the reasons for this is that there are many different sign languages, each with its own unique grammar and vocabulary. For example, American Sign Language (ASL) is a distinct language with its own grammatical rules and vocabulary, and it is not interchangeable with British Sign Language (BSL) or Australian Sign Language (Auslan). This means that individuals who are fluent in one sign language may not be able to understand or communicate effectively with individuals who use a different sign language. Another reason that sign languages are not universally comprehensible is that they are influenced by the surrounding culture and language. For example, ASL is heavily influenced by French Sign Language (LSF), which was brought to the United States by French teachers of the deaf in the 19th century [6]. Similarly, BSL is influenced by the surrounding English language and culture. This means that sign languages can vary significantly from region to region, even within the same country.

Sign language has a complex and intricate structure that differs from spoken languages. While spoken languages rely on sound, sign languages use visual cues, such as handshapes, movement, and facial expressions, to convey meaning. Sign languages also have their own grammar and syntax, which dictate how signs are arranged and used in sentences. For example, in ASL, word order is determined by the topic and comment structure. The topic is established at the beginning of the sentence and is often accompanied by a raised eyebrow or head nod. The comment follows the topic and provides additional information.

Understanding the syntax of sign language is important for gaining a deeper understanding of how the language functions and for developing effective ways of teaching and learning the language.

One of the fundamental features of sign language syntax is the use of non-manual markers. Non-manual markers are facial expressions, head movements, and other non-manual cues that accompany signs and help to convey meaning. For example, in American Sign Language (ASL), raising the eyebrows can indicate a question, while tilting the head can indicate a conditional clause. Non-manual markers are an essential part of sign language syntax, and signers must learn to use them effectively in order to convey their intended meaning.

Another important feature of sign language syntax is the use of space. signers use space to establish reference, distinguish between topics and comments, and indicate the relationship between entities in a sentence [7]. For example, in ASL, signers might use different locations in space to indicate the subject, object, and verb in a sentence. The use of space in sign language syntax can be quite complex, and signers must learn to use space effectively in order to convey meaning clearly.

Sign language syntax also involves the use of grammatical categories, such as verbs, nouns, adjectives, and pronouns [8]. Like spoken languages, sign languages have their own unique set of grammatical rules and patterns. For example, in ASL, verbs often occur at the beginning or end of a sentence, while nouns and pronouns are often located in the middle. Signers must learn to use these grammatical categories effectively in order to produce grammatically correct sentences. Hence, understanding the syntax and rules of sign language is essential for developing proficiency in the language. Sign languages are complex and rich, and signers must learn to use a wide range of linguistic features, including non-manual markers, space, and grammatical categories, in order to communicate effectively.

Sign language has gained popularity over the years and is now given the same linguistic status as of spoken languages. However, this was not the case earlier: the deaf and vocally impaired individuals were considered as disabled. To explore this, it is essential to dive into the history of sign languages and see how they originated [9, 10, 11, 12, 13]. 

Sign languages have a long and fascinating history that spans centuries and continents. While the exact origins of sign language are unclear, it is clear that sign languages have been used by Deaf communities around the world for many years. One of the earliest recorded uses of sign language dates back to the 5th century BC, when Socrates wrote about a group of Deaf people who used signs to communicate. In the centuries that followed, sign languages continued to be used by Deaf communities in many different parts of the world, including Europe, Asia, and the Americas. However, these early sign languages were often considered to be "inferior" to spoken languages, and were not widely recognized as complete and legitimate languages. [9]

It was not until the 18th century that sign language began to be recognized as a distinct and complete language. In 1755, the French physician Charles-Michel de l'Epée founded the first school for Deaf children in Paris. De l'Epée recognized the value of sign language as a means of communication for Deaf people, and developed a method of teaching French Sign Language (LSF) to his students. His work helped to establish sign language as a legitimate means of communication, and paved the way for the development of other sign languages around the world. [10, 11] Over the years, many different sign languages have emerged in different parts of the world. One of the most well-known sign languages is American Sign Language (ASL), which developed in the United States in the early 19th century. ASL was heavily influenced by French Sign Language, which was brought to the United States by Thomas Hopkins Gallaudet, a minister who was seeking to establish a school for Deaf children. Gallaudet learned LSF from one of de l'Epée's former students, and used this knowledge to help develop ASL. [12]

Other sign languages have emerged in different parts of the world, often as a result of Deaf communities coming together and developing their own unique forms of communication. For example, British Sign Language (BSL) developed in the United Kingdom in the late 19th century, and has since become widely used by Deaf people in the UK and in other parts of the world. In recent years, sign languages have gained increasing recognition as distinct and complete languages. The United Nations has recognized sign languages as official languages, and many countries have recognized sign language as an official language or have developed legislation to protect the rights of Deaf people. Sign language research has also advanced significantly, with many studies investigating the syntax, semantics, and pragmatics of sign languages. [13] In conclusion, the history of sign languages is long and complex, spanning many centuries and continents. Sign languages have played a vital role in the lives of Deaf people around the world, and have been the subject of much research and study. While there is still much to learn about sign languages, their rich history and unique linguistic properties make them a fascinating and important subject of study.

Despite the recognition of sign language as a legitimate form of communication, it has often been stigmatized and dismissed as an inferior form of communication. Many Deaf people have faced discrimination and prejudice due to their use of sign language, and have been denied access to education and other opportunities. In recent years, sign language has gained popularity and is now given the same linguistic status as of spoken languages. However, this has caused the deaf and vocally impaired sign language users to be considered as members of a cultural and linguistic minority group instead of a disability group. With such a bipartite distinction, it is a challenge for the disabled signers to identify themselves and weave smoothly into the social fabric [5]. American sign language (ASL) has grown popular in the United states and the English speaking regions of Canada; however, limited resources have engendered cultural and social problems for the deaf and vocally impaired community [14].  

Without access to proper resources and support, individuals who are deaf or mute may struggle to communicate effectively with others and may face isolation and exclusion from society. One of the most significant challenges faced by deaf and mute individuals is the lack of access to education in sign language. Many schools and educational institutions do not provide adequate resources or training for sign language, which can make it difficult for individuals to communicate effectively in academic settings. This can lead to lower academic performance and limited career opportunities for deaf and mute individuals. [15]

In addition to education, access to healthcare is another area where deaf and mute individuals may struggle due to the lack of resources in sign language. Medical professionals may not be trained in sign language, which can make it difficult for individuals to communicate their health concerns or receive appropriate treatment. This can lead to serious health consequences and may exacerbate existing health conditions [16, 17]. The lack of access to legal resources in sign language is also a significant concern for deaf and mute individuals as well. In many legal proceedings, individuals may not have access to sign language interpreters or other resources to help them communicate effectively. This can lead to misunderstandings and may result in unfair legal outcomes for deaf and mute individuals [18, 19].

Another challenge faced by deaf and mute individuals is the lack of access to social resources in sign language. Socializing and connecting with others is an essential aspect of human life, but it can be difficult for individuals who cannot communicate effectively with others. Without access to resources such as sign language interpreters or communication devices, deaf and mute individuals may struggle to make friends and may feel isolated and excluded from social activities. Additionally, the lack of resources in sign language can impact the mental health of deaf and mute individuals. Communication difficulties can lead to feelings of frustration, loneliness, and isolation, which can exacerbate mental health issues such as anxiety and depression. Without access to proper support and resources, deaf and mute individuals may struggle to manage their mental health and may not receive the care they need. [20, 21]

The challenges faced by deaf and mute individuals due to the lack of resources in sign language are significant and far-reaching. It is essential for individuals and organizations to work together to provide access to education, healthcare, legal resources, and social support for deaf and mute individuals. This includes training and support for sign language interpreters and other communication devices, as well as the development of new technologies that can support communication and access to resources for deaf and mute individuals.

The traditional method of sign language interpretation involves a human interpreter who translates the sign language into spoken language and vice versa. However, this process can be time-consuming, expensive, and unreliable, especially in cases where there is a shortage of qualified interpreters. Therefore, the need for technological advancements in the field of sign language interpretation is essential. The use of technology can provide faster and more reliable interpretation services, reduce costs, and increase access to interpretation services, especially in areas where interpreters are scarce.

Several advances have been made in the area of sign language interpretation technology. One of the most significant breakthroughs is the development of machine learning algorithms that can recognize and translate sign language into spoken language. The algorithms work by analyzing videos of sign language and identifying specific hand and facial movements. The information is then translated into spoken language using natural language processing techniques. [22] Another significant development is the creation of sign language recognition gloves. These gloves are equipped with sensors that can detect hand and finger movements and translate them into text or spoken language. This technology has been particularly helpful in situations where sign language interpreters are not available, such as in emergencies or remote areas. [23]

Moreover, advancements have also been made in the area of sign language avatars. These avatars are computer-generated images that can replicate human-like movements, including sign language gestures. Sign language avatars can be used to provide interpretation services in areas where interpreters are not available or to supplement interpretation services provided by human interpreters. In addition to the above-mentioned advancements, mobile applications that provide sign language interpretation services are also available. These applications work by analyzing videos of sign language and translating them into text or spoken language. They are particularly useful for providing interpretation services in situations where human interpreters are not available.

Despite these advancements, there is still much work to be done to improve sign language interpretation technology. For instance, some of the current technologies have limitations, such as recognizing only a limited range of sign language gestures. Furthermore, the cost of some of these technologies is still prohibitively high, making them inaccessible to many people who need them.

In conclusion, technological advancements have the potential to revolutionize the way sign language interpretation is done. These advancements can reduce barriers in access to education, healthcare, legal services, and other areas. The advances made in the area of sign language interpretation technology, such as machine learning algorithms, sign language recognition gloves, sign language avatars, and mobile applications, have shown great promise. However, there is still much work to be done to improve the efficiency, reliability, and accessibility of these technologies. Therefore, it is essential to continue investing in research and development to improve sign language interpretation technology and make it accessible to everyone who needs it.

To increase awareness and reduce the disparate impact of the society on the disabled, we propose a system that can translate from the oral modality to a verbal modality in an inclusive manner that allows a wide variety of actual words to be conveyed. There are a plethora of approaches that one can use in order to reduce the gap between the hearing impaired and the hearing abled. Existing approaches can be classified into sensor based and vision based methods. In sign language translation, a sensor-based approach uses sensors to capture hand and body movements and translate them into text or speech. This approach is commonly used in glove-based systems, where the user wears a glove with sensors that track their hand movements. On the other hand, a vision-based approach uses cameras or other visual sensors to capture video of the signer, and then uses computer vision techniques to track and recognize the hand and body movements. This approach can be used without requiring the user to wear any special equipment, making it more accessible. Both approaches have their advantages and disadvantages. Sensor-based approaches can be more accurate in tracking subtle hand movements, but they can be uncomfortable or cumbersome for the user to wear. Vision-based approaches can be more convenient for the user, but they may struggle to recognize signs in poor lighting conditions or with complex backgrounds.

Our work focuses on vision based approaches and takes into account a variety of aspects which are generally ignored in automatic sign language interpretation. We make use of the latest vision transformers and deep neural networks to create a sign language interpreter which can interpret words from a video. The difference between sign language and verbal message is that, text can be read as a sequence of words while signs can be read as sequence of frames. However, multiple frames can combine to form one word in a sign language scenario. Hence, sign language interpretation becomes a challenging task. Sentences are a sequence of a sequence that forms a word. Our work, however, focuses on building a system that recognizes individual words from a video from the WLASL dataset.  We explore the effect of passing the features extracted from a truncated pretrained ResNet50 model and poses extracted from the MoveNetV2 [24] model into a Video Vision Transformer (ViViT) [25].












LITERATURE SURVEY

Sign language is a visual language used by the deaf and hard-of-hearing community for communication. As a result, sign language interpretation has become increasingly important in bridging the communication gap between the hearing and non-hearing communities. However, manual interpretation is time-consuming and may not always be accurate. To address these challenges, researchers have explored various approaches to automatically interpret sign language. In this section, we will discuss different ways in which sign language can be automatically interpreted using technology. We will focus on existing work and do an in depth survey of literature simultaneously. From a broad perspective, existing work can be classified into two distinct buckets: sensor based approaches and vision based approaches. 

Sensor Based Approaches
Sensor-based approaches for sign language recognition involve the use of various sensors, such as gloves, EMG, and accelerometers, to capture the movements and gestures made during signing. These sensors capture data, such as joint angles, acceleration, and muscle activity, which are then processed to recognize and interpret the signs being performed. Diving deeper into sensor based approaches, we find that there are 3 major sensors that govern the world of sign language recognition. First, being the leap motion sensor, second being sensor gloves, and third being Electromyography.

The Leap Motion sensor is a small device that uses infrared cameras and LED lights to track hand and finger movements with high accuracy and low latency. It is designed to be used with computers and virtual reality systems, enabling natural and intuitive interaction through gestures and movements. In the context of sign language recognition, the Leap Motion sensor has been used to capture and interpret the hand and finger movements of sign language users, allowing for real-time translation of signs into spoken or written language. Its high precision and low latency make it a popular choice for researchers and developers working on sign language recognition and other applications that require precise hand tracking. However, the sensor has some limitations, including a limited tracking range and sensitivity to lighting conditions, which can affect its accuracy and reliability in certain settings.

C. K. M. Lee et al [26], propose a new feature extraction method based on a temporal pyramid pooling technique that captures both the temporal and spatial features of sign language gestures. The RNN model is trained with gradient clipping and L2 regularization, and the dataset used for consists of 26 English alphabet signs. in terms of recognition accuracy, the model acheives an accuracy rate of 99.31%. In "Exploiting Recurrent Neural Networks and Leap Motion Controller for the Recognition of Sign Language and Semaphoric Hand Gestures" by D. Avola et al., the authors propose a sign language recognition system using a Leap Motion controller and a Recurrent Neural Network (RNN) architecture. The system can recognize 25 different gestures of the Italian Sign Language alphabet and some additional semaphoric hand gestures, achieving a recognition accuracy of up to 97.50% [27]. R. Gouider et al [28], in another approach, proposed a sign language recognition system using the Leap Motion controller and K-means clustering. The system is designed to recognize Arabic Sign Language (ArSL) in real-time. Hands were segmented into ROIs, and the features extracted in each ROI was passed through a K means cluster. The final classification was carried out by an SVM classifier. The system achieved an accuracy of 92.5% in recognizing Arabic Sign Language (ArSL) with a vocabulary of 14 words. Lastly, M. I. B. Shamsul [29] et al propose a methodology that involves preprocessing of the input data, including the removal of background and noise reduction. Feature extraction is performed using the Scale Invariant Feature Transform (SIFT) algorithm, followed by Principal Component Analysis (PCA) for dimensionality reduction. The classification is done using the k-Nearest Neighbor (k-NN) algorithm and Support Vector Machine (SVM) with a radial basis function (RBF) kernel. The results show that the proposed system performs well in recognizing both static and dynamic signs, achieving a recognition rate of 92.5% for static signs and 75.8% for dynamic signs.

Approaches using leap motion sensors have a few drawbacks, specific to the sensor. The sensor has some limitations, including a limited tracking range and sensitivity to lighting conditions, which can affect its accuracy and reliability in certain settings. For instance, one of the systems [29], requires the user to have a clear view of the sensor and to perform the signs in a specific area, which may limit its usability in real-world scenarios. Secondly, leap motion controllers are limited to recognizing only hand gestures and cannot recognize facial expressions or body movements, which are essential in sign language interpretation. Finally, the system proposed by R. Gouider et al [28] required a clean environment and the signer to wear a colored glove to improve the accuracy of hand segmentation, which may not be ideal in real life scenarios.

Moving on, there are approaches that make use of sensor gloves. Glove sensors-based approach for sign language recognition involves the use of a special type of gloves that are embedded with sensors to capture the hand movements and gestures of the signer. These gloves are designed to measure various parameters such as the orientation and position of fingers, wrist, and forearm. The captured data is then processed by machine learning algorithms to recognize sign language gestures. This approach offers several advantages such as high accuracy, user-friendliness, and real-time recognition. However, it also has some limitations such as the requirement of wearing a special glove and the lack of flexibility in hand movements due to the presence of sensors on the glove. 

C. Oz and M. C. Leu [30] presents a system that uses a sensory glove to recognize American Sign Language (ASL) words. The system comprises a sensory glove that records hand movements, which are then translated into features that represent the hand posture. These features are fed into an artificial neural network (ANN) to recognize ASL words. The authors conducted experiments to evaluate the system's performance using a dataset of ASL words. The results showed that the system achieved an average recognition rate of 92.6% for ten ASL words. N. Bajaj et al [31] propose a system for real-time recognition and translation of Indian Sign Language (ISL) using a sensor-based approach. The system utilizes an array of sensors consisting of accelerometers, gyroscopes, and magnetometers to capture the motion of the signer's hand and wrist. The captured motion data is then preprocessed and fed into a deep neural network for recognition and translation. The results show that the proposed system achieves high accuracy in recognizing and translating ISL signs in real-time. The authors report an average recognition accuracy of 95.64% and an average translation accuracy of 87.39%. In another approach, J. K. Perez et al [32], proposed a low-cost glove-based system for real-time sign language translation. The system uses flex sensors and an accelerometer to capture hand movements and gestures, which are then processed and recognized using a support vector machine (SVM) classifier. The system is designed to recognize 26 American Sign Language (ASL) alphabets, numbers, and common phrases. The system achieved an average recognition accuracy of 94% for the 26 signs and phrases, and a recognition accuracy of 100% for the ASL alphabets. The system proposed by J. K Perez [32] is robust to variation in lightenings. Lastly, S. Ghosh et al [33] proposed a system that uses a glove equipped with flex sensors and accelerometers to capture hand and finger movements during signing. The data from the sensors is preprocessed and then fed into a fuzzy inference system to recognize ASL gestures. The system was evaluated using a dataset of 25 commonly used ASL gestures and achieved an accuracy of 90.4%. 

Approaches using wearable glove sensors have a few drawbacks. One major limitation is the discomfort and inconvenience of wearing a glove for extended periods of time, which can deter some signers from using the system. Additionally, the sensors in the glove may not be sensitive enough to accurately capture the subtle nuances of certain signs. The glove may also not be able to capture non-manual markers such as facial expressions and body posture, which are important components of sign language. Finally, the cost of the glove-based system can be prohibitive for many users, particularly those who may not have access to funding or insurance coverage. A few papers from the literature [30, 32], elaborate on how the system may have difficulty recognizing signs and phrases when there is significant variation in how they are signed by different people. 

The EMG (Electromyography) approach to recognize sign languages involves placing electrodes on the skin to detect the electrical signals generated by muscle movements in the arm and hand. These signals are then processed to identify specific hand and finger movements associated with sign language gestures. This approach has the advantage of being non-invasive and able to detect subtle muscle movements, but it may be limited by the need for precise electrode placement and potential interference from other muscle activity. Additionally, it may not be suitable for individuals with mobility impairments that affect their ability to move their arms and hands.

S. S. Mahajan et al [34] collected sEMG data from 20 subjects performing hand and finger gestures, and used feature extraction techniques to extract time-domain, frequency-domain, and time-frequency-domain features from the sEMG signals. The extracted features were used as input to various machine learning algorithms, including decision tree, random forest, k-nearest neighbor, and support vector machine (SVM), to train and evaluate the performance of the gesture recognition system. The results show that SVM performed the best among the tested machine learning algorithms, achieving an average recognition accuracy of 93.5% for hand gestures and 91.2% for finger gestures. R. Zangeneh et al [35] propose a system for sign language recognition using surface electromyography (EMG) signals and artificial neural networks (ANNs). The study focuses on recognizing the American Sign Language (ASL) alphabet. The results of the study show that the proposed system achieved an accuracy of 93.8% for recognizing the ASL alphabet. K. Adarsh et al [36] propose a novel approach for sign language recognition using electromyography (EMG) signals and convolutional neural networks (CNNs). The study involves the collection of EMG signals from the forearm muscles of the user while performing American Sign Language (ASL) gestures. The signals are preprocessed and segmented into windows to extract features using the discrete wavelet transform (DWT) and the principle component analysis (PCA) techniques. The extracted features are then fed into a CNN model to classify the corresponding ASL gestures. The results indicate that the proposed approach achieves an accuracy of 92.3%. S. Shaghayegh et al [37] present an approach for recognizing Persian sign language (PSL) using EMG signals and Hidden Markov Models (HMMs). The results showed that the proposed approach achieved an accuracy of 86.4% for discrete HMMs and 90.2% for continuous HMMs for recognizing 23 PSL signs.

Vision Based Approaches
A vision-based approach for sign language recognition, on the other hand, uses cameras and computer vision algorithms to analyze and interpret hand gestures and movements. The system captures images or videos of the signer's hands and then processes them to identify and classify the sign language gestures. This approach does not require the use of gloves or sensors and can be implemented using a variety of camera systems, including webcams, mobile phone cameras, and depth cameras. However, the accuracy of vision-based systems can be affected by lighting conditions, occlusion, and variations in hand shapes and sizes. Vision based systems focus on 2 types of signings: static and dynamic. Static involve images of signs while dynamic involves videos of signers signing more complicated classes of words.

Let’s focus on static signing first. In "User-Independent American Sign Language Alphabet Recognition Based on Depth Image and PCANet Features," [38] the authors propose a new approach to recognize the American Sign Language alphabet using depth images and PCANet features. The proposed system consists of three stages: pre-processing, feature extraction, and classification. In the pre-processing stage, the authors use background subtraction and hand segmentation to extract the hand region from the depth image. Then, in the feature extraction stage, PCANet features are extracted from the depth image. Finally, in the classification stage, the authors use a support vector machine (SVM) classifier to recognize the American Sign Language alphabet.  The results show that the proposed system achieves a recognition rate of 93.1% for user-independent recognition but is dependent on a depth sensor, which can be expensive and not widely available. R. Ahuja et al. [39] present a system for recognizing static hand gestures of American Sign Language (ASL) using convolutional neural networks (CNN). The authors collected a dataset of 8700 images for 29 ASL hand signs and used it to train a CNN model. They experimented with different CNN architectures and found that a modified version of the LeNet-5 architecture achieved the best performance. The system achieved an accuracy of 99.19% on the test set and was able to recognize ASL gestures in real time. R. Kaluri et al [40], propose a framework for recognizing sign language gestures using a combination of adaptive histogram equalization (AHE) and hidden Markov models (HMMs). The methodology involves preprocessing the input images using AHE, followed by feature extraction using the local binary pattern (LBP) algorithm. The HMMs are trained using a maximum likelihood estimation (MLE) algorithm, and the Viterbi algorithm is used for decoding. The approach achieves a recognition rate of 93.1%. Lastly, V. Bheda et al [41] propose a method for American Sign Language (ASL) gesture recognition using deep convolutional networks (DCNs). They use a publicly available ASL dataset and a pre-trained DCN model and fine-tune the model for ASL recognition. They also perform data augmentation to improve the model's robustness to variation in hand positions and lighting conditions. The authors achieved an overall accuracy of 94.45% on the test set. 

The issue with static images is that they cater to a very small percentage of the sign language corpus. Majority of the sign language involve complex motions that static images cannot encapsulate. Hence, we need to consider videos of signers as input to a system istead of static images for more robust recognition. There is existing research work in the direction of continuous signing. In the following paragraph we will discuss exactly that.

In "Scalable ASL Sign Recognition using Model-based Machine Learning and Linguistically Annotated Corpora," [42] the authors propose a novel approach to sign language recognition that uses model-based machine learning and linguistically annotated corpora to improve recognition accuracy. Feature extraction involves extracting features from the motion capture and video data, including joint positions and orientations, hand shape, and facial expressions. The models are trained using a combination of discriminative and generative models, and the recognition process involves selecting the most likely sign based on the input data. The video dataset used here is the ASLLVD dataset [43]. Besides this, J. Huang et al [44] propose a method for recognizing sign language gestures using 3D convolutional neural networks.The methodology involves collecting depth images of sign language gestures and converting them into 3D voxel representations. These voxel representations are fed into a 3D CNN architecture consisting of multiple convolutional and pooling layers, followed by fully connected layers for classification. The authors evaluate the performance of their method on the RWTH-BOSTON-104 database [45]. In a comparatively novel approach, Hua Huang et al [46] propose a method for recognizing Chinese Sign Language (CSL) using convolutional neural networks (CNNs) and temporal pyramid matching. The authors use a dataset consisting of over 16,000 sign language samples, which are preprocessed by extracting key points from the depth image data. The proposed method first trains a CNN to recognize static hand gestures, followed by a temporal CNN to capture the temporal dynamics of sign language. Experimental results show that the proposed method achieves an accuracy of 98.7% on the CSL dataset. Lastly, Thanh Tung Doan et al [47] propose a deep learning-based method for American Sign Language (ASL) recognition using spatiotemporal features from video data. The method consists of two main stages: hand segmentation and recognition. The hand segmentation is performed using a deep learning-based approach, which detects the hand region in each frame of the input video. The recognition stage involves extracting spatiotemporal features from the segmented hand region using a 3D convolutional neural network (CNN). The authors achieved a recognition accuracy of 97.06%. 

Research Gaps
Considering the extensive survey of literature, there are certain gaps in research that can be laid out as follows.

Firstly, the data used by multitudes of papers is very less compared to the existing vocabulary in sign language. A lot of the papers propose trivial image recognition solutions that allow only recognition of alphabets. This lack of vocabulary majorly restricts the system's capability. Furthermore, IOT devices and sensors (cyber gloves, etc) give good results but are unable to generalize to different hand sizes when passed through artificial neural networks. Single signer based datasets also face the same problem. Over and above this, it is difficult to incorporate large variations in hand gestures to allow translations to a wide variety of words. It is also not viable to have users wear gloves or attach EMG sensors t themselves while signing. Other than comfort, cost is also a major factor. Over and above this, hardware comes with an increased failure rate and softwares are much more scalable and practical. To overcome the above issues, vision based approaches exist, but static image based solutions do not give much room for recognition of words that are a result of vivid hand gestures. Hence, there have been strides in considering temporal data to recognize videos of signers signing in sign language. The limitations in these, however, is that they use older sequence modeling techniques like RNN and LSTMs for the same. Sequence modeling is restricted to Hidden Markov Models, Recurrent neural networks, and LSTMs only and the latest technologies like Transformers are not considered as a modeling option when training the data. 

Problem Statement
Automatic sign language recognition systems can be categorized into three classes, namely sentence, words, and fingerspelling recognition. This paper focuses on word classification. The Problem statement is to translate short videos of signers conveying a single word to a word in the english corpus of length 2000. This paper however, focuses on vision based methods on WLASL100 and WLASL300 (top 100 and 300 glossary words in the WLASL dataset with 2000 classes). WLASL is a large-scale dataset for Word-Level American Sign Language.

The research objective is to cater to a larger corpus of words and take into consideration newer modelling techniques like MoveNet (published in 2021) [48] and Video Vision Transformers (published in 2021) [49] to attack the sequential nature of videos.

































METHODOLOGY

Overview
To approach automatic recognition of sign language, two experiments are conducted. Before diving into the details of the experiments, a generic idea of the approach goes as follows. The input to the system is a video of signers signing, where frames of the videos are stacked in ascending order of time. The 3 dimensional input is then passed through a pretrained feature extractor to extract the most important features from the frame images. A CNN layer succeeding the feature extractor, learns a little bit more over the pretrained network, and the output of this CNN layer acts as an input to the video vision transformer (ViViT). The video vision transformer models the frame features in a sequential manner to recognize the sign. Figure 1 shows the generic workflow. Different experiments indicate different feature extractors.


Fig 1: Generic flow diagram for video sign recognition 

In this work, the focus has been on translating the American Sign Language (ASL). As described above, sign languages are entire languages in themselves and different sign languages are not mutually nor universally comprehensible. Hence, for each sign language, a model similar to the one defined above can be trained. American Sign Language (ASL) is a visual-gestural language used primarily by deaf and hard-of-hearing individuals in the United States and parts of Canada. It has its own grammatical rules and syntax, and uses a combination of hand gestures, facial expressions, and body language to convey meaning. American Sign Language (ASL) is similar to other sign languages in that it is a visual-gestural language that is used by Deaf communities to communicate. ASL has similarities to other sign languages, such as French Sign Language, British Sign Language, and Australian Sign Language, as they share some signs and grammatical features. However, one significant difference between ASL and other sign languages is the degree of finger spelling. ASL uses finger spelling extensively, whereas some other sign languages use it sparingly. Additionally, ASL is distinct from other sign languages in its use of facial expressions and body language, which play a crucial role in conveying meaning.

Dataset
To execute the proposed approach, it is important to have a dataset that supports the input assumptions of the system. A video of signers signing different words in ASL are required. For this we use the WLASL dataset (A large-scale dataset for Word-Level American Sign Language). The WLASL dataset is a large-scale video dataset that contains 3,624 American Sign Language (ASL) signs performed by 76 signers. It was created to support the development of sign language recognition technology using computer vision and machine learning techniques. The dataset has several unique features, including high variability in signers' demographics, clothing, and camera viewpoints, making it more challenging for recognition algorithms. It also has temporal annotations for each sign, allowing for the evaluation of both recognition accuracy and sign-level temporal segmentation. The WLASL dataset is freely available for research purposes and has already been used in various studies, including the development of deep learning models for sign language recognition.

In 2020, Dongxu Li et al in the paper titled “Word-level Deep Sign Language Recognition from Video: A New Large-scale Dataset and Methods Comparison”, addressed the challenges faced by existing ASL datasets and built the current largest dataset for the same. Comprising more than 2000 words from over 100 signers, the dataset allows researchers to explore machine learning models that are not limited to only alphabets and digits. Moreover, the dataset contains video signing, allowing the models to consider the motion of hands during translation. [50]

Figure 2 shows two examples from the WLASL dataset (The top row is an ASL sequence for the world ‘rice’, while the bottom row is an ASL sequence for ‘soup’)


Fig 2: frames from the WLASL dataset for rice (top) and soup (bottom). Source: WLASL [50]

Data Preparation
The WLASL dataset needs heavy preprocessing for it to be a fair dataset for the chosen approach. The preprocessing required, and the reasons behind it are discussed in this section. Data preparation is divided into two parts: 1. Video length compression and frame resizing and 2. Data Augmentation. 

First, let’s discuss video length compression. It was observed that each video had varied lengths and hence varied number of frames. The videos were shot at a rate of 25 frames per second (fps) and modeling would require a constant and minimal number of frames per video.  It was also noticed that very few frames were required to encapsulate the sequence. Hence, to eradicate the bias based on length of videos, the videos were either slowed down or sped up to an approximate duration of 3 seconds (giving us a constant 75 frames). Figure 3 shows the video duration before and after this preprocessing step. Out of the 75 frames, 25 equidistant frames were chosen for modeling. Each frame was converted to grayscale and resized to a size of (256,256).


Fig 3: Shows the duration of videos before (left) and after (right) video length compression.

Once 25 frames were chosen from the video, it was important to cater to the class imbalance using data augmentation. Data augmentation is a technique used in machine learning and computer vision to increase the size of a training dataset by creating modified copies of the existing data.

It was observed that over 500 videos had 5 videos per class and over 400 videos had 6 videos per class. To fix the class imbalance, we augmented each video such that we have a minimum of 10 videos per class. Augmentation involves controlled rotation, zooming, flipping and contrast variations. Figure 4 shows skew in number of videos per word vs number of words having those many numbers of videos.



Fig 4: Number of videos per word compared to number of words before data augmentation.

Introduction to Transformers
Transformers are a type of neural network architecture that was introduced in 2017. They are designed for sequence-to-sequence modeling tasks such as language translation, question answering, and text summarization. Unlike traditional recurrent neural networks (RNNs) that process inputs sequentially, transformers process the entire input sequence in parallel, which makes them more efficient and easier to train.

Transformers were introduced in a paper titled “Attention is all you need” in 2017 [51]. The Transformer architecture consists of an encoder and decoder component, each of which contains a stack of identical layers. Each layer has two sub-layers: a multi-head self-attention mechanism and a feedforward network. The self-attention mechanism computes a weighted sum of the input sequence, where the weights are determined by a learned attention function that takes into account the relationships between all pairs of positions in the input sequence. The feedforward network applies a position-wise transformation to the self-attention output, which is then passed to the next layer. During training, the model is optimized to predict the next token in a sequence given the previous tokens, and during inference, the model generates sequences autoregressively. Figure 5 shows a generic transformer architecture.

Vision Transformers (ViT)
Vision Transformers (ViT) is a recently proposed method for image classification that leverages the power of Transformers. While Convolutional Neural Networks (CNNs) have been the dominant approach for image classification, ViT seeks to explore a new way to process images using self-attention mechanisms. Unlike CNNs, ViT processes images as sequences of patches and uses a self-attention mechanism to capture the relationships between patches. The self-attention mechanism enables ViT to focus on the most important patches for classification, allowing it to be more efficient and accurate than traditional CNNs. Vision transformers were 

Fig 5: Transformer architecture

introduced in the paper “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale” by Alexey et al in June 2021. [52] Vision transformers explore the effects of multihead attention on images by dividing it into 16x16 patches and encoding them with position embeddings before passing it through transformer blocks, where it goes through multi head self attention. Figure 6 shows the general architecture of a Vision Transformer.

Vision transformers (ViT) are not the best when it comes to having scarce data. This is exactly the case with the WLASL dataset. Convolution neural networks (CNN), on the other hand, perform well with less data. However, ViT has the ability to embody a larger amount of global information than models like ResNet at the initial layers, leading to comparatively distinct features, and the skip connections in a vision transformer are even more dominant than in Residual Networks, having strong effects on performance. Therefore we combine both CNNs and ViTs for modeling purposes. CNNs come into picture when dealing with feature extraction in our proposed method.

The video vision transformer used in this paper is highly motivated by vision transformers, hence, it must also be noted that the benefits and drawbacks of vision transformers can be extended to Video Vision Transformers since they are heavily based on Vision Transformers. 


Fig 6: Architecture of a vision transformer

Video Vision Transformers
Video Vision Transformer (ViViT) is an extension of the Vision Transformer (ViT) model for video processing tasks such as action recognition, video classification, and video segmentation. The model processes input frames individually through the ViT architecture and then aggregates the output of each frame using temporal attention mechanisms to produce a final video-level prediction. ViViT has achieved state-of-the-art performance on various video datasets and has shown to be computationally efficient compared to traditional 3D CNN-based models.

Based on the success of ViTs, Anurag et al proposed ViViT (A video vision transformer) in 2021 [49]. The pure transformer architecture extracts spatio-temporal tokens from the input video, which are then embedded by a series of transformer layers. The authors experiment with factorizing the spatial and temporal dimensions of the input in different ways. The architecture outline for the video vision transformer is almost the same as figure 6. The only difference is factorizing and how the input video is embedded into tokens. 

Tubelet embeddings are used to embed the video input. It not only divides the image into spatial patches but also includes temporal information in it. The volume frames are then linearly projected and are added with position embeddings before being passed to the transformer layers. Figure 7 shows how an input video is embedded using tubelet embeddings.



Fig 7: How tubelet embedding embeds spatio-temporal information. (Source: ViViT [49])

It must be noted that in our implementation of the ViViT, we make use of 8 transformer encoder layers, a patch size of (8,8,8), 8 heads for attention and a linear projection dimension of 128. 

Experiment 1 - Truncated Resnet and a Shrinker Network
"Deep Residual Learning for Image Recognition" by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun introduced the Residual Network (ResNet) architecture in 2016. ResNet is a deep neural network architecture that enables the construction of deeper neural networks, which were previously difficult to train due to the vanishing gradient problem. ResNet solves this problem by introducing residual connections, allowing gradient to flow directly through the network's layers. In the paper, the authors demonstrated the effectiveness of ResNet on the ImageNet dataset, achieving state-of-the-art results. They also showed that ResNet can be used as a feature extractor and finetuned for various downstream tasks, such as object detection and semantic segmentation. [53]

A truncated pretrained ResNet50 gave the perfect middle ground for less computation but efficient feature extraction. Since the pretrained model was trained on imagenet, it could extract basic important features efficiently from each frame. The output of the model was (25, 32, 32, 512) for each video and it was too huge for the video vision transformer to process given the memory and computing constraints. Hence, we introduce a Shrinker Network, which shrinks the ResNet output to (25, 32,32,1). The outputs are stacked together before sending it to the video vision transformer. Figure 8 shows the rough architecture of the system for experiment 1 while figure 9 shows the detailed architecture of the Shrinker network.


Fig 8: Rough architecture for experiment 1 (ResNet50)




Fig 9: Detailed architecture for the shrinker network


Experiment 2 - Movenet thunder and an Expander Network
MoveNet Thunder is the latest advancement in joint angle profile predictions. The model uses heatmaps to localize human key points. The architecture consists of a MobileNetV2 [54] based features extractor, a feature pyramid network and a set of prediction heads. It was proposed by google’s tensorflow in 2021 and beats state of the art models when it comes to fast accurate pose extraction. The architecture of the MoveNet model is given in figure 10. We make use of tensorflow’s pretrained MoveNet Thunder model which outputs a set of 17 poses. Figure 11 shows the result of applying MoveNet to frames in the WLASL dataset. 


Fig 10: Architecture of the MoveNet Model used for feature extraction in Experiment 2


Fig 11: Result of MoveNet on frames from the WLASL dataset

The output of te MoveNet model is a (17,2) feature vector determining the pose. The video vision transformer however takes images of minimum (16,16). Hence, we introduce an expander model which basically blows up the (17,2) output of the MoveNet to (17,17). We train the expander network such that it takes as input an image of shape (Batch_size, 25,17,2) and outputs an image of shape (Batch_size, 25, 17, 17), which would be passed to the ViViT model for classification. Note that 25 is the number of frames in one video input. 

The rough architecture of the system is given in figure 12, while the architecture of the Expander Network is given in figure 13.


Fig 12: Rough Architecture for experiment 2 (MoveNet)



Fig 13: Architecture of the expander network





IMPLEMENTATION

Configuration
EMBED_DIM = 100
FF_DIM = 32
OUTPUT_SIZE = 100
POSITION_EMBEDDING_SIZE = 10
BATCH_SIZE = 4
VIDEO_SEQUENCE_LENGTH=25
LR_DECAY_RATE=0.96
LR_DECAY_STEPS=100000

AUTO = tf.data.AUTOTUNE
INPUT_SHAPE = (25, 17, 17, 1)
NUM_CLASSES = 100

# OPTIMIZER
LEARNING_RATE = 1e-5
WEIGHT_DECAY = 1e-5

# TRAINING
EPOCHS = 60

# TUBELET EMBEDDING
PATCH_SIZE = (8, 8, 8)
NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2

# ViViT ARCHITECTURE
LAYER_NORM_EPS = 1e-6
PROJECTION_DIM = 128
NUM_HEADS = 8
NUM_LAYERS = 8

PRINT_WIDTH=48

vid_path = "../input/aslamericansignlanguagewlaslpreprocessed"
d100 = "../input/wlasl-divided/WLASL_full/WLASL_full/WLASL100/"
d300 = "../input/wlasl-divided/WLASL_full/WLASL_full/WLASL300/"

d100_train = d100+"train/"
d100_dev = d100+"dev/"
d100_test = d100+"test/"

d100_train_tot = 700
d100_dev_tot = 100
d100_test_tot = 320

d300_train = d300+"train/"
d300_dev = d300+"dev/"
d300_test = d300+"test/"

d300_train_tot = 1800
d300_dev_tot = 300
d300_test_tot = 728


Data Generator
def data_generator(tot, i, path):
    '''
    aim of this would be to output (Batch size, 74, 256, 256)
    '''
    bx_index = i//16
    x = load_data(path+"bx"+str(bx_index)+".npy")
    y = load_data(path+"by"+str(bx_index)+".npy")
    if i+BATCH_SIZE <= tot:
        return i+BATCH_SIZE, x[(i-(16*bx_index)):(i-(16*bx_index))+BATCH_SIZE], y[(i-(16*bx_index)):(i-(16*bx_index))+BATCH_SIZE]
    else:
        return i+BATCH_SIZE, x[(i-(16*bx_index)):tot], y[(i-(16*bx_index)):tot]


Resnet - Exp 1
def build_feature_extractor():
    base_model = ResNet50(weights='imagenet', include_top=True, input_tensor=Input(shape=(256, 256, 3)))
    ResNet14 = Model(inputs = base_model.input,outputs = base_model.get_layer('conv3_block4_add').output)
    ResNet14.trainable = False
    return ResNet14
m = build_feature_extractor()


Shrinker Network
class Shrinker(Model):
    def __init__(self):
        super(Expander, self).__init__()
        self.conv1 = Conv2D(256, kernel_size=(3, 3), strides=(1,1), padding='SAME')
        self.conv2 = Conv2D(80, kernel_size=(3, 3), strides=(1,1), padding='SAME')
        self.conv3 = Conv2D(64, kernel_size=(3, 3), strides=(1,1), padding='SAME')
        self.conv4 = Conv2D(32, kernel_size=(3, 3), strides=(1,1), padding='SAME')
        self.conv5 = Conv2D(1, kernel_size=(1, 1), strides=(1,1), padding='SAME')
        self.bn1 = BatchNormalization()
        self.bn2 = BatchNormalization()
        self.bn3 = BatchNormalization()
        self.bn4 = BatchNormalization()
        self.act1 = Activation('relu')
        self.act2 = Activation('relu')
        self.act3 = Activation('relu')
        self.act4 = Activation('relu')
        self.act5 = Activation('relu')
    
    def call(self, inputs):
        x = self.conv2(inputs)
        x = self.act2(x)
        x = self.bn2(x)
        x = self.conv3(x)
        x = self.act3(x)
        x = self.bn3(x)
        x = self.conv4(x)
        x = self.act4(x)
        x = self.bn4(x)
        x = self.conv5(x)
        x = self.act5(x)
        return x


Movenet - Exp 2
model_name = "movenet_thunder_int8"

if "tflite" in model_name:
  if "movenet_lightning_f16" in model_name:
    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/float16/4?lite-format=tflite
    input_size = 192
  elif "movenet_thunder_f16" in model_name:
    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/float16/4?lite-format=tflite
    input_size = 256
  elif "movenet_lightning_int8" in model_name:
    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/int8/4?lite-format=tflite
    input_size = 192
  elif "movenet_thunder_int8" in model_name:
    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/int8/4?lite-format=tflite
    input_size = 256
  else:
    raise ValueError("Unsupported model name: %s" % model_name)

  # Initialize the TFLite interpreter
  interpreter = tf.lite.Interpreter(model_path="model.tflite")
  interpreter.allocate_tensors()

  def movenet(input_image):
    input_image = tf.cast(input_image, dtype=tf.uint8)
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    interpreter.set_tensor(input_details[0]['index'], input_image.numpy())
    interpreter.invoke()
    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])
    return keypoints_with_scores

else:
  if "movenet_lightning" in model_name:
    module = hub.load("https://tfhub.dev/google/movenet/singlepose/lightning/4")
    input_size = 192
  elif "movenet_thunder" in model_name:
    module = hub.load("https://tfhub.dev/google/movenet/singlepose/thunder/4")
    input_size = 256
  else:
    raise ValueError("Unsupported model name: %s" % model_name)

  def movenet(input_image):
    model = module.signatures['serving_default']
    input_image = tf.cast(input_image, dtype=tf.int32)
    outputs = model(input_image)
    keypoints_with_scores = outputs['output_0'].numpy()
    return keypoints_with_scores


Expander Network
class Expander(Model):
    def __init__(self):
        super(Expander, self).__init__()
        self.conv1 = Conv2D(256, kernel_size=(3, 3), strides=(1,1), padding='SAME')
        self.conv6 = Conv2D(256, kernel_size=(3, 3), strides=(1,1), padding='SAME')
        self.conv2 = Conv2D(128, kernel_size=(3, 3), strides=(1,1), padding='SAME')
        self.conv3 = Conv2D(64, kernel_size=(3, 3), strides=(1,1), padding='SAME')
        self.conv4 = Conv2D(32, kernel_size=(3, 3), strides=(1,1), padding='SAME')
        self.conv5 = Conv2D(17, kernel_size=(3, 3), strides=(1,1), padding='SAME')
        self.bn1 = BatchNormalization()
        self.bn2 = BatchNormalization()
        self.bn3 = BatchNormalization()
        self.bn4 = BatchNormalization()
        self.bn5 = BatchNormalization()
        self.act1 = Activation('relu')
        self.act2 = Activation('relu')
        self.act3 = Activation('relu')
        self.act4 = Activation('relu')
        self.act5 = Activation('relu')
        self.act6 = Activation('relu')
    
    def call(self, inputs):
        x = self.conv1(inputs)
        x = self.act1(x)
        x = self.bn1(x)
        
        x = self.conv6(x)
        x = self.act6(x)
        x = self.bn5(x)
        
        x = self.conv2(x)
        x = self.act2(x)
        x = self.bn2(x)
        
        x = self.conv3(x)
        x = self.act3(x)
        x = self.bn3(x)
        
        x = self.conv4(x)
        x = self.act4(x)
        x = self.bn4(x)
        
        x = self.conv5(x)
        x = self.act5(x)
        return x


Vision Transformer
class TubeletEmbedding(layers.Layer):
    def __init__(self, embed_dim, patch_size, **kwargs):
        super().__init__(**kwargs)
        self.projection = layers.Conv3D(
            filters=embed_dim,
            kernel_size=patch_size,
            strides=patch_size,
            padding="VALID",
        )
        self.flatten = layers.Reshape(target_shape=(-1, embed_dim))

    def call(self, videos):
        projected_patches = self.projection(videos)
        flattened_patches = self.flatten(projected_patches)
        return flattened_patches
    
class PositionalEncoder(layers.Layer):
    def __init__(self, embed_dim, **kwargs):
        super().__init__(**kwargs)
        self.embed_dim = embed_dim

    def build(self, input_shape):
        _, num_tokens, _ = input_shape
        self.position_embedding = layers.Embedding(
            input_dim=num_tokens, output_dim=self.embed_dim
        )
        self.positions = tf.range(start=0, limit=num_tokens, delta=1)

    def call(self, encoded_tokens):
        # Encode the positions and add it to the encoded tokens
        encoded_positions = self.position_embedding(self.positions)
        encoded_tokens = encoded_tokens + encoded_positions
        return encoded_tokens
    
class ViViT(Model):
    def __init__(self):
        super(ViViT, self).__init__()
        self.tubelet_embedder=TubeletEmbedding(embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE)
        self.positional_encoder = PositionalEncoder(embed_dim=PROJECTION_DIM)
        self.ln1 = layers.LayerNormalization(epsilon=1e-6)
        self.ln2 = layers.LayerNormalization(epsilon=1e-6)
        self.ln3 = layers.LayerNormalization(epsilon=1e-6)
        self.ln4 = layers.LayerNormalization(epsilon=1e-6)
        self.ln5 = layers.LayerNormalization(epsilon=1e-6)
        self.ln6 = layers.LayerNormalization(epsilon=1e-6)
        self.ln7 = layers.LayerNormalization(epsilon=1e-6)
        self.ln8 = layers.LayerNormalization(epsilon=1e-6)
        self.ln9 = layers.LayerNormalization(epsilon=1e-6)
        self.ln10 = layers.LayerNormalization(epsilon=1e-6)
        self.ln11 = layers.LayerNormalization(epsilon=1e-6)
        self.ln12 = layers.LayerNormalization(epsilon=1e-6)
        self.ln13 = layers.LayerNormalization(epsilon=1e-6)
        self.ln14 = layers.LayerNormalization(epsilon=1e-6)
        self.ln15 = layers.LayerNormalization(epsilon=1e-6)
        self.ln16 = layers.LayerNormalization(epsilon=1e-6)
        self.lnfinal = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)
        self.multihead1 = layers.MultiHeadAttention(num_heads=NUM_HEADS, key_dim=PROJECTION_DIM // NUM_HEADS, dropout=0.1)
        self.multihead2 = layers.MultiHeadAttention(num_heads=NUM_HEADS, key_dim=PROJECTION_DIM // NUM_HEADS, dropout=0.1)
        self.multihead3 = layers.MultiHeadAttention(num_heads=NUM_HEADS, key_dim=PROJECTION_DIM // NUM_HEADS, dropout=0.1)
        self.multihead4 = layers.MultiHeadAttention(num_heads=NUM_HEADS, key_dim=PROJECTION_DIM // NUM_HEADS, dropout=0.1)
        self.multihead5 = layers.MultiHeadAttention(num_heads=NUM_HEADS, key_dim=PROJECTION_DIM // NUM_HEADS, dropout=0.1)
        self.multihead6 = layers.MultiHeadAttention(num_heads=NUM_HEADS, key_dim=PROJECTION_DIM // NUM_HEADS, dropout=0.1)
        self.multihead7 = layers.MultiHeadAttention(num_heads=NUM_HEADS, key_dim=PROJECTION_DIM // NUM_HEADS, dropout=0.1)
        self.multihead8 = layers.MultiHeadAttention(num_heads=NUM_HEADS, key_dim=PROJECTION_DIM // NUM_HEADS, dropout=0.1)
        self.add1 = layers.Add()
        self.add2 = layers.Add()
        self.add3 = layers.Add()
        self.add4 = layers.Add()
        self.add5 = layers.Add()
        self.add6 = layers.Add()
        self.add7 = layers.Add()
        self.add8 = layers.Add()
        self.add9 = layers.Add()
        self.add10 = layers.Add()
        self.add11 = layers.Add()
        self.add12 = layers.Add()
        self.add13 = layers.Add()
        self.add14 = layers.Add()
        self.add15 = layers.Add()
        self.add16 = layers.Add()
        self.densea1 = layers.Dense(units=PROJECTION_DIM * 4, activation=tf.nn.gelu)
        self.densea2 = layers.Dense(units=PROJECTION_DIM * 4, activation=tf.nn.gelu)
        self.densea3 = layers.Dense(units=PROJECTION_DIM * 4, activation=tf.nn.gelu)
        self.densea4 = layers.Dense(units=PROJECTION_DIM * 4, activation=tf.nn.gelu)
        self.densea5 = layers.Dense(units=PROJECTION_DIM * 4, activation=tf.nn.gelu)
        self.densea6 = layers.Dense(units=PROJECTION_DIM * 4, activation=tf.nn.gelu)
        self.densea7 = layers.Dense(units=PROJECTION_DIM * 4, activation=tf.nn.gelu)
        self.densea8 = layers.Dense(units=PROJECTION_DIM * 4, activation=tf.nn.gelu)
        self.denseb1 = layers.Dense(units=PROJECTION_DIM, activation=tf.nn.gelu)
        self.denseb2 = layers.Dense(units=PROJECTION_DIM, activation=tf.nn.gelu)
        self.denseb3 = layers.Dense(units=PROJECTION_DIM, activation=tf.nn.gelu)
        self.denseb4 = layers.Dense(units=PROJECTION_DIM, activation=tf.nn.gelu)
        self.denseb5 = layers.Dense(units=PROJECTION_DIM, activation=tf.nn.gelu)
        self.denseb6 = layers.Dense(units=PROJECTION_DIM, activation=tf.nn.gelu)
        self.denseb7 = layers.Dense(units=PROJECTION_DIM, activation=tf.nn.gelu)
        self.denseb8 = layers.Dense(units=PROJECTION_DIM, activation=tf.nn.gelu)
        self.densefinal = layers.Dense(units=NUM_CLASSES, activation="softmax")
        self.gap = layers.GlobalAveragePooling1D()
    
    def call(self, inputs):
        patches = self.tubelet_embedder(inputs)
        encoded_patches = self.positional_encoder(patches)
        
        x1 = self.ln1(encoded_patches)
        attention_output = self.multihead1(x1,x1)
        x2 = self.add1([attention_output, encoded_patches])
        x3 = self.ln2(x2)
        x3 = self.densea1(x3)
        x3 = self.denseb1(x3)
        encoded_patches = self.add2([x3,x2])
        
        x1 = self.ln3(encoded_patches)
        attention_output = self.multihead2(x1,x1)
        x2 = self.add3([attention_output, encoded_patches])
        x3 = self.ln4(x2)
        x3 = self.densea2(x3)
        x3 = self.denseb2(x3)
        encoded_patches = self.add4([x3,x2])
        
        x1 = self.ln5(encoded_patches)
        attention_output = self.multihead3(x1,x1)
        x2 = self.add5([attention_output, encoded_patches])
        x3 = self.ln6(x2)
        x3 = self.densea3(x3)
        x3 = self.denseb3(x3)
        encoded_patches = self.add6([x3,x2])
        
        x1 = self.ln7(encoded_patches)
        attention_output = self.multihead4(x1,x1)
        x2 = self.add7([attention_output, encoded_patches])
        x3 = self.ln8(x2)
        x3 = self.densea4(x3)
        x3 = self.denseb4(x3)
        encoded_patches = self.add8([x3,x2])
        
        x1 = self.ln9(encoded_patches)
        attention_output = self.multihead5(x1,x1)
        x2 = self.add9([attention_output, encoded_patches])
        x3 = self.ln10(x2)
        x3 = self.densea5(x3)
        x3 = self.denseb5(x3)
        encoded_patches = self.add10([x3,x2])
        
        x1 = self.ln11(encoded_patches)
        attention_output = self.multihead6(x1,x1)
        x2 = self.add11([attention_output, encoded_patches])
        x3 = self.ln12(x2)
        x3 = self.densea6(x3)
        x3 = self.denseb6(x3)
        encoded_patches = self.add12([x3,x2])
        
        x1 = self.ln13(encoded_patches)
        attention_output = self.multihead7(x1,x1)
        x2 = self.add13([attention_output, encoded_patches])
        x3 = self.ln14(x2)
        x3 = self.densea7(x3)
        x3 = self.denseb7(x3)
        encoded_patches = self.add14([x3,x2])
        
        x1 = self.ln15(encoded_patches)
        attention_output = self.multihead8(x1,x1)
        x2 = self.add15([attention_output, encoded_patches])
        x3 = self.ln16(x2)
        x3 = self.densea8(x3)
        x3 = self.denseb8(x3)
        encoded_patches = self.add16([x3,x2])
        
        representation = self.lnfinal(encoded_patches)
        representation = self.gap(representation)
        outputs = self.densefinal(representation)
        return outputs
        
Train Loop
def train(epochs, train_data_path, dev_data_path, vivit,expander, total_train, total_dev):
    for e in range(epochs):
        
        print("epoch:",e)
        
        st = time.time() #start timer        
        idx = 0
        loss_train=0
        acc_train = (0.0, 0.0, 0.0)
        while idx<total_train-BATCH_SIZE:
            if idx%PRINT_WIDTH == 0:
                print("batch t:", str(idx), end=" ")
                
            with tf.GradientTape() as tape:
                idx, x, y = data_generator(total_train, idx, train_data_path)
                o = expander_model(x)
                o = tf.expand_dims(o,axis=4)
                outputs = vivit(o)
                ll, loss = loss_function(y,outputs)
                acc = accuracy(y, outputs)
                acc_train = tuple(map(lambda x, y: x+(y*BATCH_SIZE), acc_train, acc))
            loss_train+=loss
            variables = vivit.trainable_variables + expander.trainable_variables
            gradients = tape.gradient(loss,variables)                        
            optimizer.apply_gradients(zip(gradients, variables))
        total_loss_train.append(loss_train/total_train)
        acc_train = [i/total_train for i in acc_train]
        total_acc_train.append(acc_train)
        
        print()
        print("train loss:", str(loss_train/total_train))
        print("train accuracy:", str(acc_train))
        print("time:", time.time()-st)
        print()
        
        st = time.time()
        idx = 0
        loss_dev=0
        acc_dev = (0.0, 0.0, 0.0)
        while idx<total_dev:
            if idx%PRINT_WIDTH == 0:
                print("batch d:", str(idx), end=" ")
            idx, x, y = data_generator(total_dev, idx, dev_data_path)
            o = expander_model(x)
            o = tf.expand_dims(o,axis=4)
            outputs = vivit(o)
            ll, loss = loss_function(y, outputs)
            acc = accuracy(y, outputs)
            acc_dev = tuple(map(lambda x, y: x+(y*BATCH_SIZE), acc_dev, acc))
            loss_dev+=loss
        total_loss_val.append(loss_dev/total_dev)
        acc_dev = [i/total_dev for i in acc_dev]
        total_acc_val.append(acc_dev)
    
        print()
        print("dev loss:", str(loss_dev/total_dev))
        print("dev accuracy:", str(acc_dev))
        print("time:", time.time()-st)
        print()
        print("===================================================================================================================================")
        print()

    return vivit, expander, total_acc_train, total_acc_val, total_loss_val, total_loss_train















RESULTS

Experimental Setup
Table 1 shows the experimental setup for execution of the experiments


CPU RAM
13 GB
GPU RAM
16 GB
Graphics Processor
Tesla T4
Language
Python 3.8.1


Table 1: Experimental Setup

Results
Both experiment 1 (Truncated ResNet50) and experiment 2 (MoveNet Thunder) perform well on the WLASL100 and WLASL300 dataset, however it is seen that the ResNet50 variation fails to generalize well. Although the training curve surpasses few of the baseline models described in the paper introducing the WLASL dataset, the validation and test results don’t compliment it. However, experiment 2 seems to be comparatively promising and performs well on all train, dev and test datasets. Experiment 1 gives a train accuracy of 58.72% while giving a test accuracy of 43.25% on the WLASL100 dataset. Experiment 2 on the other hand gives a train accuracy of 61.38% and a test accuracy of 59.44% on the WLASL100 dataset. As we move on to WLASL 300, the trends remain the same whereas the accuracies drop as a result of increased number of target classes. 


Method
WLASL100 (%)
WLASL300 (%)


Top-1
Top-5
Top-10
Top-1
Top-5
Top-10
Exp 1 (train)
58.72
83.84
91.35
41.92
69.22
80.07
Exp 1 (test)
43.25
70.79
79.15
30.14
55.03
68.47
Exp 2 (train)
61.38
80.36
90.03
51.40
74.06
85.68
Exp 2 (test)
59.44
78.97
89.23
48.38
72.59
86.81


Table 2: Train and Test accuracies on WLASL100 and WLASL300 for experiment 1 (denoted by Exp1) and experiment 2 (denoted by Exp2)



For training, the loss function was set to be the usual categorical cross entropy. A learning rate of 1e-5 was used and an exponential learning rate decay with a decay rate of 0.96 and a decay step of 1e5 was applied. For experiment 2, a batch size of 16 was used due to it being computationally cost effective while a batch size of 4 was used in experiment 1, which was computationally heavy. Furthermore, it is to be noted that both the experiments are performed in the computational constraints defined by Table 1 and the results may be dependent on them.


Fig 14: Training curve for the Movenet variant (experiment 2)



FIg 15: Training curve for the ResNet Variant (experimet 1)


The training curve for experiment 2 is shown in figure 14 and the top-1, top-2 and top-10 accuracies on the train and test datasets are displayed in table 2. The MoveNet variant does better than Pose-GRU, Pose-TGCN and VGG-GRU and the evidence for the same is shown in table 3. The training curve for experiment 1 is shown in figure 15.


Method
WLASL100 (%)
WLASL300 (%)


Top-1
Top-5
Top-10
Top-1
Top-5
Top-10
Pose-GRU
46.51
76.74
85.66
33.68
64.37
76.05
Pose-TGCN
55.43
78.68
87.60
38.32
67.51
79.64
VGG-GRU
25.97
55.04
63.95
19.31
46.56
61.08
Exp 1 (test)
43.25
70.79
79.15
30.14
55.03
68.47
Exp 2 (test)
59.44
78.97
89.23
48.38
72.59
86.81


Table 3: Comparing results of previous state of the art models on the WLASL100 and WLASL300 dataset with test accuracies of experiment 1 (denoted by Exp1) and experiment 2 (denoted by Exp2).





















CONCLUSION

We propose a new efficient method for user independent translation from ASL to english words. A deep learning approach to vision based translation is employed to translate a video feed into a set of known english words. The main objective of the paper is to explore rising deep learning technologies like pose detection using MoveNet and video classification using video vision transformers.

The paper experiments using two feature extractors (MoveNet Thunder and a truncated pretrained ResNet50) and introduces for each of the experiments an expander and a shrinker network which allow us to leverage the spatiotemporal advantages of video vision transformers in a computation friendly manner.

While the ResNet50 model fails to generalize well, the MoveNet variant predicts with an accuracy better than previous state of the art models like Pose-GRU, Pose-TGCN and VGG-GRU. The MoveNet model achieves a top-1 accuracy of 59.44% for the WLASL100 dataset and a top-1 accuracy of 48.38% for the WLASL300 dataset.

Future work involves training in a computational environment friendly enough to train the WLASL1000 and WLASL2000 subsets of the WLASL dataset. Furthermore, it is also possible to experiment with different patch sizes and transformer embedder layers to see its effects on the results.


















REFERENCES

[1]	B. J. Kröger, P. Birkholz, J. Kannampuzha, E. Kaufmann, and I. Mittelberg, ‘Movements and holds in fluent sentence production of American Sign Language: The action-based approach’, Cognitive computation, vol. 3, no. 3, pp. 449–465, 2011

[2]	N. Gupta and N. Jhanwar, "A Comparative Study of Body Language and Sign Language," International Journal of Computer Science and Mobile Computing, vol. 4, no. 4, pp. 200-207, Apr. 2015

[3]	E. Wilkinson and K. S. Schmitz, "The Role of Facial Expressions in Sign Language and Body Language," Sign Language Studies, vol. 18, no. 4, pp. 558-576, 2018.

[4]	K. Goudarzi and K. Tsivola, "Cross-Cultural Differences in Body Language and Sign Language," Journal of Language and Social Psychology, vol. 36, no. 3, pp. 317-330, Jun. 2017

[5]	J. Napier and L. Leeson, ‘Sign language in action’, in Sign language in action, Springer, 2016, pp. 50–84

[6]	Padden, C. A., & Humphries, T. L. (2005). A brief history of American Sign Language. In D. F. Armstrong, M. A. Karchmer, & J. V. Van Cleve (Eds.), The study of signed languages: Essays in honor of William C. Stokoe (pp. 3-24). Gallaudet University Press

[7]	D. Lillo-Martin, "The Syntax of American Sign Language: Functional Categories and Hierarchical Structure," in Natural Language & Linguistic Theory, vol. 10, no. 3, pp. 395-429, Sep. 1992, doi: 10.1007/BF00992726

[8]	K. Emmorey, "Syntactic and Semantic Factors in the Use of Space in American Sign Language," in Language and Cognitive Processes, vol. 10, no. 1, pp. 31-56, 1995, doi: 10.1080/01690969508407076

[9]	Bahan, B., & Bauman, H. D. (2018). American Sign Language. In Oxford Research Encyclopedia of Linguistics. doi: 10.1093/acrefore/9780199384655.013.204.

[10]	de Quadros, R. M. (2014). Sign languages of the world: A comparative handbook. Cambridge University Press

[11]	Emmorey, K. (2002). Language, cognition, and the brain: Insights from sign language research. Psychology Press

[12]	H. Lane, "When the mind hears: A history of t